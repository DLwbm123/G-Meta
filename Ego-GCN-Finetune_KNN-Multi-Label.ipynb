{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "class Subgraphs(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.subgraph_list[self.data.iloc[index]['name']], self.subgraph2label[self.data.iloc[index]['name']], self.subgraph2center_node[self.data.iloc[index]['name']] \n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels, center_nodes = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.LongTensor(labels), torch.LongTensor(center_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subgraphs_test(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node, k_shot, n_qry):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "        self.k_shot = k_shot\n",
    "        self.n_qry = n_qry\n",
    "        \n",
    "        self.labels = np.unique(self.data.label.values)\n",
    "        self.labels_dict = dict(zip(list(self.labels), list(range(len(self.labels)))))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        support = []\n",
    "        query = []\n",
    "        \n",
    "        for i in self.labels:\n",
    "            df_labels = self.data[self.data.label == i].reset_index(drop = True)\n",
    "            support = support + list(df_labels.sample(n = self.k_shot)['name'].values)\n",
    "            query = query + list(df_labels[~df_labels.name.isin(support)].sample(n = self.n_qry)['name'].values)\n",
    "        \n",
    "        return [self.subgraph_list[i] for i in support], [self.labels_dict[self.subgraph2label[i]] for i in support], [self.subgraph2center_node[i] for i in support], [self.subgraph_list[i] for i in query], [self.labels_dict[self.subgraph2label[i]] for i in query], [self.subgraph2center_node[i] for i in query]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 100 \"batches\"\n",
    "        return 100\n",
    "    \n",
    "def collate_test(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    \n",
    "    graphs, labels, center_nodes, g_qry, l_qry, c_qry = map(list, zip(*samples))\n",
    "    batched_graph_spt = dgl.batch(graphs[0])\n",
    "    batched_graph_qry = dgl.batch(g_qry[0])\n",
    "    \n",
    "    return batched_graph_spt, torch.LongTensor(labels), torch.LongTensor(center_nodes), batched_graph_qry, torch.LongTensor(l_qry), torch.LongTensor(c_qry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, to_fetch, features):\n",
    "        # For undirected graphs, in_degree is the same as\n",
    "        # out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float().to(device)\n",
    "        #h = torch.tensor([1.]*g.number_of_nodes()).view(-1, 1).float()\n",
    "\n",
    "        #h = features.float()\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        #print(h.shape)\n",
    "        #hg = dgl.mean_nodes(g, 'h')\n",
    "        #print(to_fetch)\n",
    "        num_nodes_ = g.batch_num_nodes\n",
    "        num_nodes_.insert(0, 0)\n",
    "        offset = torch.cumsum(torch.LongTensor(num_nodes_), dim = 0)[:-1].to(device)\n",
    "        hg = h[to_fetch + offset]\n",
    "        #print(hg.shape)\n",
    "        #print(hg.shape)\n",
    "        #print(h[0].shape)\n",
    "        #hg = h[g.nodes[0].data['center_node'].detach().numpy()[0]]\n",
    "        #print(hg.shape)\n",
    "        return hg, self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "path = './data/multiple_graph/cycle/META_LABEL/random_edge100/'\n",
    "with open(path + 'list_subgraph.pkl', 'rb') as f:\n",
    "    total_subgraph = pickle.load(f)\n",
    "    \n",
    "with open(path + 'label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "with open(path + 'center.pkl', 'rb') as f:\n",
    "    center_node = pickle.load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Finetune(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        super(Classifier_Finetune, self).__init__()\n",
    "\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, model, g, to_fetch, features):\n",
    "        \n",
    "        h, _ = model(g, to_fetch, features)\n",
    "        \n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold_n):\n",
    "    path = './data/multiple_graph/cycle/META_LABEL/random_edge100/'\n",
    "\n",
    "    path = path + 'fold'+str(fold_n)+'/'\n",
    "    trainset = Subgraphs(path, 'train', total_subgraph, info, center_node)\n",
    "    valset = Subgraphs(path, 'val', total_subgraph, info, center_node)\n",
    "    testset = Subgraphs_test(path, 'test', total_subgraph, info, center_node, 1, 12)\n",
    "\n",
    "    data_loader_test = DataLoader(testset, batch_size=1, shuffle=True, collate_fn=collate_test)\n",
    "\n",
    "    data_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model = Classifier(1, 256, max(info.values()) + 1)\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    print('model training')\n",
    "    for epoch in range(50):\n",
    "        epoch_loss = 0\n",
    "        for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "            bg = bg.to(device)\n",
    "            label = label.to(device)\n",
    "            features = 1\n",
    "            hid, prediction = model(bg, to_fetch, features)\n",
    "            #print(prediction.shape)\n",
    "            loss = loss_func(prediction, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        \n",
    "        if epoch%25 ==0:\n",
    "            print('Epoch {}, training loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    \n",
    "    print('model finetuning')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        model_finetune = Classifier_Finetune(256, 2)\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "\n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,)    \n",
    "\n",
    "        update_step = 10\n",
    "        loss_steps = []\n",
    "        acc_steps = []\n",
    "\n",
    "        for i in range(update_step):\n",
    "            model_finetune.train()\n",
    "            bg_ = copy.deepcopy(bg)\n",
    "            prediction = model_finetune(model, bg_, to_fetch, 1)\n",
    "            loss = loss_func(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model_finetune.eval()\n",
    "            bg_ = copy.deepcopy(bg_qry)\n",
    "            pred_qry = model_finetune(model, bg_, to_fetch_qry, 1)\n",
    "            probs_Y = torch.softmax(pred_qry, 1)\n",
    "            argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "            label_q = torch.tensor(label_qry).float().view(-1, 1)\n",
    "            acc = (label_q == argmax_Y.detach().cpu().float()).sum().item() / len(label_q) * 100\n",
    "\n",
    "            acc_steps.append(acc)\n",
    "            loss_steps.append(loss.item())\n",
    "        accs.append(acc_steps)\n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "    print(np.mean(np.array(accs), 0))\n",
    "    \n",
    "    \n",
    "    print('model KNN')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "        \n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,) \n",
    "        \n",
    "        bg_ = copy.deepcopy(bg)\n",
    "        h, prediction = model(bg_, to_fetch, 1)\n",
    "        \n",
    "        bg_ = copy.deepcopy(bg_qry)\n",
    "        h_qry, pred_qry = model(bg_, to_fetch_qry, 1)\n",
    "        \n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        dist = pairwise_distances(h.detach().numpy(), h_qry.detach().numpy())\n",
    "        \n",
    "        y_pred = np.argmin(dist, 0).reshape(-1,)\n",
    "        \n",
    "        label_dict = dict(zip(label.numpy(), list(range(label.numpy().reshape(-1,).shape[0]))))\n",
    "        y_pred = np.array([label_dict[i] for i in y_pred])\n",
    "        #print(y_pred.shape)\n",
    "        label_q = label_qry.float().view(-1, ).numpy()\n",
    "\n",
    "        acc = sum(label_q == y_pred) / len(label_q) * 100\n",
    "\n",
    "        accs.append(acc)\n",
    "        \n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "            \n",
    "    print(np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.9445\n",
      "Epoch 25, training loss 1.0477\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[51.375      51.83333333 52.5        54.20833333 54.875      55.95833333\n",
      " 56.54166667 58.54166667 60.20833333 61.58333333]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "85.375\n"
     ]
    }
   ],
   "source": [
    "run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.9366\n",
      "Epoch 25, training loss 1.0458\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[53.45833333 53.625      54.83333333 55.625      56.625      57.625\n",
      " 59.83333333 61.58333333 62.625      64.79166667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "84.5\n"
     ]
    }
   ],
   "source": [
    "run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.9064\n",
      "Epoch 25, training loss 0.9786\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[47.66666667 48.29166667 49.5        52.29166667 55.33333333 60.375\n",
      " 63.29166667 66.66666667 68.83333333 70.58333333]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "82.58333333333334\n"
     ]
    }
   ],
   "source": [
    "run(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.8962\n",
      "Epoch 25, training loss 1.0351\n",
      "model finetuning\n",
      "step  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[48.16666667 48.16666667 48.29166667 48.125      48.70833333 48.83333333\n",
      " 49.         50.25       51.25       51.625     ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "74.58333333333334\n"
     ]
    }
   ],
   "source": [
    "run(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.9217\n",
      "Epoch 25, training loss 0.9944\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[50.         49.91666667 50.04166667 49.875      50.41666667 50.75\n",
      " 51.79166667 51.5        51.29166667 51.08333333]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "49.58333333333334\n"
     ]
    }
   ],
   "source": [
    "run(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BA(fold_n):\n",
    "    path = './data/multiple_graph/BA/META_LABEL/'\n",
    "    \n",
    "    with open(path + 'list_subgraph.pkl', 'rb') as f:\n",
    "        total_subgraph = pickle.load(f)\n",
    "    \n",
    "    with open(path + 'label.pkl', 'rb') as f:\n",
    "        info = pickle.load(f)\n",
    "\n",
    "    with open(path + 'center.pkl', 'rb') as f:\n",
    "        center_node = pickle.load(f)\n",
    "    \n",
    "    path = path + 'fold'+str(fold_n)+'/'\n",
    "    trainset = Subgraphs(path, 'train', total_subgraph, info, center_node)\n",
    "    valset = Subgraphs(path, 'val', total_subgraph, info, center_node)\n",
    "    testset = Subgraphs_test(path, 'test', total_subgraph, info, center_node, 1, 12)\n",
    "\n",
    "    data_loader_test = DataLoader(testset, batch_size=1, shuffle=True, collate_fn=collate_test)\n",
    "\n",
    "    data_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model = Classifier(1, 256, max(info.values()) + 1)\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    print('model training')\n",
    "    for epoch in range(50):\n",
    "        epoch_loss = 0\n",
    "        for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "            bg = bg.to(device)\n",
    "            label = label.to(device)\n",
    "            features = 1\n",
    "            hid, prediction = model(bg, to_fetch, features)\n",
    "            #print(prediction.shape)\n",
    "            loss = loss_func(prediction, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        \n",
    "        if epoch%25 ==0:\n",
    "            print('Epoch {}, training loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    \n",
    "    print('model finetuning')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        model_finetune = Classifier_Finetune(256, 2)\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "\n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,)    \n",
    "\n",
    "        update_step = 10\n",
    "        loss_steps = []\n",
    "        acc_steps = []\n",
    "\n",
    "        for i in range(update_step):\n",
    "            model_finetune.train()\n",
    "            bg_ = copy.deepcopy(bg)\n",
    "            prediction = model_finetune(model, bg_, to_fetch, 1)\n",
    "            loss = loss_func(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model_finetune.eval()\n",
    "            bg_ = copy.deepcopy(bg_qry)\n",
    "            pred_qry = model_finetune(model, bg_, to_fetch_qry, 1)\n",
    "            probs_Y = torch.softmax(pred_qry, 1)\n",
    "            argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "            label_q = torch.tensor(label_qry).float().view(-1, 1)\n",
    "            acc = (label_q == argmax_Y.detach().cpu().float()).sum().item() / len(label_q) * 100\n",
    "\n",
    "            acc_steps.append(acc)\n",
    "            loss_steps.append(loss.item())\n",
    "        accs.append(acc_steps)\n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "    print(np.mean(np.array(accs), 0))\n",
    "    \n",
    "    \n",
    "    print('model KNN')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "        \n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,) \n",
    "        \n",
    "        bg_ = copy.deepcopy(bg)\n",
    "        h, prediction = model(bg_, to_fetch, 1)\n",
    "        \n",
    "        bg_ = copy.deepcopy(bg_qry)\n",
    "        h_qry, pred_qry = model(bg_, to_fetch_qry, 1)\n",
    "        \n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        dist = pairwise_distances(h.detach().numpy(), h_qry.detach().numpy())\n",
    "        \n",
    "        y_pred = np.argmin(dist, 0).reshape(-1,)\n",
    "        \n",
    "        label_dict = dict(zip(label.numpy(), list(range(label.numpy().reshape(-1,).shape[0]))))\n",
    "        y_pred = np.array([label_dict[i] for i in y_pred])\n",
    "        #print(y_pred.shape)\n",
    "        label_q = label_qry.float().view(-1, ).numpy()\n",
    "\n",
    "        acc = sum(label_q == y_pred) / len(label_q) * 100\n",
    "\n",
    "        accs.append(acc)\n",
    "        \n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "            \n",
    "    print(np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.7288\n",
      "Epoch 25, training loss 0.2990\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[51.83333333 53.16666667 55.08333333 56.41666667 56.5        58.16666667\n",
      " 60.41666667 63.         64.25       65.95833333]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "79.95833333333334\n"
     ]
    }
   ],
   "source": [
    "run_BA(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 2.0427\n",
      "Epoch 25, training loss 1.1083\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[51.33333333 53.         54.5        56.58333333 57.25       60.83333333\n",
      " 62.58333333 63.41666667 64.83333333 67.16666667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "94.875\n"
     ]
    }
   ],
   "source": [
    "run_BA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 2.9276\n",
      "Epoch 25, training loss 0.7290\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[47.41666667 48.125      47.625      49.5        52.375      54.70833333\n",
      " 56.375      58.75       60.         61.79166667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "53.25000000000001\n"
     ]
    }
   ],
   "source": [
    "run_BA(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.9972\n",
      "Epoch 25, training loss 0.6662\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[48.95833333 49.375      49.54166667 49.75       49.95833333 50.5\n",
      " 51.45833333 52.41666667 52.95833333 53.79166667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "59.66666666666666\n"
     ]
    }
   ],
   "source": [
    "run_BA(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 3.0090\n",
      "Epoch 25, training loss 1.0970\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[50.         50.91666667 52.70833333 54.83333333 57.16666667 58.33333333\n",
      " 60.08333333 62.79166667 64.375      66.29166667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "96.875\n"
     ]
    }
   ],
   "source": [
    "run_BA(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
