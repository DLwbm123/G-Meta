{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "class Subgraphs(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.subgraph_list[self.data.iloc[index]['name']], self.subgraph2label[self.data.iloc[index]['name']], self.subgraph2center_node[self.data.iloc[index]['name']] \n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels, center_nodes = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.LongTensor(labels), torch.LongTensor(center_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subgraphs_test(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node, k_shot, n_qry):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "        self.k_shot = k_shot\n",
    "        self.n_qry = n_qry\n",
    "        \n",
    "        self.labels = np.unique(self.data.label.values)\n",
    "        self.labels_dict = dict(zip(list(self.labels), list(range(len(self.labels)))))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        support = []\n",
    "        query = []\n",
    "        \n",
    "        for i in self.labels:\n",
    "            df_labels = self.data[self.data.label == i].reset_index(drop = True)\n",
    "            support = support + list(df_labels.sample(n = self.k_shot)['name'].values)\n",
    "            query = query + list(df_labels[~df_labels.name.isin(support)].sample(n = self.n_qry)['name'].values)\n",
    "        \n",
    "        return [self.subgraph_list[i] for i in support], [self.labels_dict[self.subgraph2label[i]] for i in support], [self.subgraph2center_node[i] for i in support], [self.subgraph_list[i] for i in query], [self.labels_dict[self.subgraph2label[i]] for i in query], [self.subgraph2center_node[i] for i in query]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 100 \"batches\"\n",
    "        return 100\n",
    "    \n",
    "def collate_test(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    \n",
    "    graphs, labels, center_nodes, g_qry, l_qry, c_qry = map(list, zip(*samples))\n",
    "    batched_graph_spt = dgl.batch(graphs[0])\n",
    "    batched_graph_qry = dgl.batch(g_qry[0])\n",
    "    \n",
    "    return batched_graph_spt, torch.LongTensor(labels), torch.LongTensor(center_nodes), batched_graph_qry, torch.LongTensor(l_qry), torch.LongTensor(c_qry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, to_fetch, features):\n",
    "        # For undirected graphs, in_degree is the same as\n",
    "        # out_degree.\n",
    "        h = g.in_degrees().view(-1, 1).float().to(device)\n",
    "        #h = torch.tensor([1.]*g.number_of_nodes()).view(-1, 1).float()\n",
    "\n",
    "        #h = features.float()\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        #print(h.shape)\n",
    "        #hg = dgl.mean_nodes(g, 'h')\n",
    "        #print(to_fetch)\n",
    "        num_nodes_ = g.batch_num_nodes\n",
    "        num_nodes_.insert(0, 0)\n",
    "        offset = torch.cumsum(torch.LongTensor(num_nodes_), dim = 0)[:-1].to(device)\n",
    "        hg = h[to_fetch + offset]\n",
    "        #print(hg.shape)\n",
    "        #print(hg.shape)\n",
    "        #print(h[0].shape)\n",
    "        #hg = h[g.nodes[0].data['center_node'].detach().numpy()[0]]\n",
    "        #print(hg.shape)\n",
    "        return hg, self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "path = './data/single_graph/cycle_random1000/META_SETUP_LABEL/'\n",
    "with open(path + 'list_subgraph.pkl', 'rb') as f:\n",
    "    total_subgraph = pickle.load(f)\n",
    "    \n",
    "with open(path + 'label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "with open(path + 'center.pkl', 'rb') as f:\n",
    "    center_node = pickle.load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Finetune(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        super(Classifier_Finetune, self).__init__()\n",
    "\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, model, g, to_fetch, features):\n",
    "        \n",
    "        h, _ = model(g, to_fetch, features)\n",
    "        \n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold_n):\n",
    "    path = './data/single_graph/cycle_random1000/META_SETUP_LABEL/'\n",
    "\n",
    "    path = path + 'fold'+str(fold_n)+'/'\n",
    "    trainset = Subgraphs(path, 'train', total_subgraph, info, center_node)\n",
    "    valset = Subgraphs(path, 'val', total_subgraph, info, center_node)\n",
    "    testset = Subgraphs_test(path, 'test', total_subgraph, info, center_node, 1, 12)\n",
    "\n",
    "    data_loader_test = DataLoader(testset, batch_size=1, shuffle=True, collate_fn=collate_test)\n",
    "\n",
    "    data_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model = Classifier(1, 256, max(info.values()) + 1)\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    print('model training')\n",
    "    for epoch in range(50):\n",
    "        epoch_loss = 0\n",
    "        for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "            bg = bg.to(device)\n",
    "            label = label.to(device)\n",
    "            features = 1\n",
    "            hid, prediction = model(bg, to_fetch, features)\n",
    "            #print(prediction.shape)\n",
    "            loss = loss_func(prediction, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        \n",
    "        if epoch%25 ==0:\n",
    "            print('Epoch {}, training loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    \n",
    "    print('model finetuning')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        model_finetune = Classifier_Finetune(256, 2)\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "\n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,)    \n",
    "\n",
    "        update_step = 10\n",
    "        loss_steps = []\n",
    "        acc_steps = []\n",
    "\n",
    "        for i in range(update_step):\n",
    "            model_finetune.train()\n",
    "            bg_ = copy.deepcopy(bg)\n",
    "            prediction = model_finetune(model, bg_, to_fetch, 1)\n",
    "            loss = loss_func(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model_finetune.eval()\n",
    "            bg_ = copy.deepcopy(bg_qry)\n",
    "            pred_qry = model_finetune(model, bg_, to_fetch_qry, 1)\n",
    "            probs_Y = torch.softmax(pred_qry, 1)\n",
    "            argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "            label_q = torch.tensor(label_qry).float().view(-1, 1)\n",
    "            acc = (label_q == argmax_Y.detach().cpu().float()).sum().item() / len(label_q) * 100\n",
    "\n",
    "            acc_steps.append(acc)\n",
    "            loss_steps.append(loss.item())\n",
    "        accs.append(acc_steps)\n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "    print(np.mean(np.array(accs), 0))\n",
    "    \n",
    "    \n",
    "    print('model KNN')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "        \n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,) \n",
    "        \n",
    "        bg_ = copy.deepcopy(bg)\n",
    "        h, prediction = model(bg_, to_fetch, 1)\n",
    "        \n",
    "        bg_ = copy.deepcopy(bg_qry)\n",
    "        h_qry, pred_qry = model(bg_, to_fetch_qry, 1)\n",
    "        \n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        dist = pairwise_distances(h.detach().numpy(), h_qry.detach().numpy())\n",
    "        \n",
    "        y_pred = np.argmin(dist, 0).reshape(-1,)\n",
    "        \n",
    "        label_dict = dict(zip(label.numpy(), list(range(label.numpy().reshape(-1,).shape[0]))))\n",
    "        y_pred = np.array([label_dict[i] for i in y_pred])\n",
    "        #print(y_pred.shape)\n",
    "        label_q = label_qry.float().view(-1, ).numpy()\n",
    "\n",
    "        acc = sum(label_q == y_pred) / len(label_q) * 100\n",
    "\n",
    "        accs.append(acc)\n",
    "        \n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "            \n",
    "    print(np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.7086\n",
      "Epoch 25, training loss 0.7484\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[52.75       52.95833333 53.08333333 54.25       55.66666667 57.75\n",
      " 60.66666667 62.91666667 65.125      66.875     ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "86.16666666666666\n"
     ]
    }
   ],
   "source": [
    "run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.7894\n",
      "Epoch 25, training loss 0.8364\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[52.70833333 53.75       54.95833333 57.70833333 60.625      62.20833333\n",
      " 66.375      69.66666667 72.04166667 73.91666667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "99.25\n"
     ]
    }
   ],
   "source": [
    "run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.7392\n",
      "Epoch 25, training loss 0.8194\n",
      "model finetuning\n",
      "step  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[46.875      47.70833333 49.08333333 49.91666667 52.08333333 54.70833333\n",
      " 57.29166667 59.625      61.79166667 63.375     ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "88.79166666666666\n"
     ]
    }
   ],
   "source": [
    "run(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.7560\n",
      "Epoch 25, training loss 0.8165\n",
      "model finetuning\n",
      "step  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[47.29166667 47.83333333 48.91666667 50.33333333 52.16666667 54.29166667\n",
      " 57.         59.375      62.45833333 65.20833333]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "86.79166666666664\n"
     ]
    }
   ],
   "source": [
    "run(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.7770\n",
      "Epoch 25, training loss 0.8516\n",
      "model finetuning\n",
      "step  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:68: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[51.04166667 51.54166667 52.45833333 54.66666667 56.75       59.625\n",
      " 62.41666667 65.58333333 68.         70.5       ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "97.83333333333331\n"
     ]
    }
   ],
   "source": [
    "run(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_BA(fold_n):\n",
    "    path = './data/single_graph/BA/META_SETUP_LABEL/'\n",
    "    \n",
    "    with open(path + 'list_subgraph.pkl', 'rb') as f:\n",
    "        total_subgraph = pickle.load(f)\n",
    "    \n",
    "    with open(path + 'label.pkl', 'rb') as f:\n",
    "        info = pickle.load(f)\n",
    "\n",
    "    with open(path + 'center.pkl', 'rb') as f:\n",
    "        center_node = pickle.load(f)\n",
    "    \n",
    "    path = path + 'fold'+str(fold_n)+'/'\n",
    "    trainset = Subgraphs(path, 'train', total_subgraph, info, center_node)\n",
    "    valset = Subgraphs(path, 'val', total_subgraph, info, center_node)\n",
    "    testset = Subgraphs_test(path, 'test', total_subgraph, info, center_node, 1, 6)\n",
    "\n",
    "    data_loader_test = DataLoader(testset, batch_size=1, shuffle=True, collate_fn=collate_test)\n",
    "\n",
    "    data_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model = Classifier(1, 256, max(info.values()) + 1)\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    print('model training')\n",
    "    for epoch in range(100):\n",
    "        epoch_loss = 0\n",
    "        for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "            bg = bg.to(device)\n",
    "            label = label.to(device)\n",
    "            features = 1\n",
    "            hid, prediction = model(bg, to_fetch, features)\n",
    "            #print(prediction.shape)\n",
    "            loss = loss_func(prediction, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        \n",
    "        if epoch%25 ==0:\n",
    "            print('Epoch {}, training loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    \n",
    "    print('model finetuning')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        model_finetune = Classifier_Finetune(256, 2)\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "\n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,)    \n",
    "\n",
    "        update_step = 10\n",
    "        loss_steps = []\n",
    "        acc_steps = []\n",
    "\n",
    "        for i in range(update_step):\n",
    "            model_finetune.train()\n",
    "            bg_ = copy.deepcopy(bg)\n",
    "            prediction = model_finetune(model, bg_, to_fetch, 1)\n",
    "            loss = loss_func(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model_finetune.eval()\n",
    "            bg_ = copy.deepcopy(bg_qry)\n",
    "            pred_qry = model_finetune(model, bg_, to_fetch_qry, 1)\n",
    "            probs_Y = torch.softmax(pred_qry, 1)\n",
    "            argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "            label_q = torch.tensor(label_qry).float().view(-1, 1)\n",
    "            acc = (label_q == argmax_Y.detach().cpu().float()).sum().item() / len(label_q) * 100\n",
    "\n",
    "            acc_steps.append(acc)\n",
    "            loss_steps.append(loss.item())\n",
    "        accs.append(acc_steps)\n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "    print(np.mean(np.array(accs), 0))\n",
    "    \n",
    "    \n",
    "    print('model KNN')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "        \n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,) \n",
    "        \n",
    "        bg_ = copy.deepcopy(bg)\n",
    "        h, prediction = model(bg_, to_fetch, 1)\n",
    "        \n",
    "        bg_ = copy.deepcopy(bg_qry)\n",
    "        h_qry, pred_qry = model(bg_, to_fetch_qry, 1)\n",
    "        \n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        dist = pairwise_distances(h.detach().numpy(), h_qry.detach().numpy())\n",
    "        \n",
    "        y_pred = np.argmin(dist, 0).reshape(-1,)\n",
    "        \n",
    "        label_dict = dict(zip(label.numpy(), list(range(label.numpy().reshape(-1,).shape[0]))))\n",
    "        y_pred = np.array([label_dict[i] for i in y_pred])\n",
    "        #print(y_pred.shape)\n",
    "        label_q = label_qry.float().view(-1, ).numpy()\n",
    "\n",
    "        acc = sum(label_q == y_pred) / len(label_q) * 100\n",
    "\n",
    "        accs.append(acc)\n",
    "        \n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "            \n",
    "    print(np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 2.4665\n",
      "Epoch 25, training loss 0.9601\n",
      "Epoch 50, training loss 0.9331\n",
      "Epoch 75, training loss 0.8742\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[52.16666667 53.5        54.5        56.25       59.         63.91666667\n",
      " 65.75       67.25       68.91666667 70.        ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "92.49999999999999\n"
     ]
    }
   ],
   "source": [
    "run_BA(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 3.2333\n",
      "Epoch 25, training loss 1.4629\n",
      "Epoch 50, training loss 1.3171\n",
      "Epoch 75, training loss 1.0693\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[49.5        49.83333333 50.91666667 52.91666667 54.16666667 54.91666667\n",
      " 56.5        59.16666667 61.16666667 62.66666667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "89.66666666666664\n"
     ]
    }
   ],
   "source": [
    "run_BA(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 2.8521\n",
      "Epoch 25, training loss 1.4942\n",
      "Epoch 50, training loss 1.3489\n",
      "Epoch 75, training loss 1.3301\n",
      "model finetuning\n",
      "step  0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[52.75       53.33333333 56.16666667 56.         57.75       57.83333333\n",
      " 58.75       58.08333333 59.58333333 60.33333333]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "68.83333333333334\n"
     ]
    }
   ],
   "source": [
    "run_BA(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 2.2035\n",
      "Epoch 25, training loss 1.0298\n",
      "Epoch 50, training loss 1.0233\n",
      "Epoch 75, training loss 0.9785\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[51.75       52.75       56.66666667 59.5        60.91666667 64.\n",
      " 65.5        67.66666667 70.41666667 72.75      ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "52.25\n"
     ]
    }
   ],
   "source": [
    "run_BA(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 2.6934\n",
      "Epoch 25, training loss 1.3136\n",
      "Epoch 50, training loss 1.2073\n",
      "Epoch 75, training loss 1.1753\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[45.         47.         48.25       50.75       54.91666667 59.33333333\n",
      " 62.75       64.         66.41666667 68.66666667]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "98.49999999999999\n"
     ]
    }
   ],
   "source": [
    "run_BA(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
