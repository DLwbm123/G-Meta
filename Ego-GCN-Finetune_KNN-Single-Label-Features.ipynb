{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "class Subgraphs(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.subgraph_list[self.data.iloc[index]['name']], self.subgraph2label[self.data.iloc[index]['name']], self.subgraph2center_node[self.data.iloc[index]['name']] \n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels, center_nodes = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.LongTensor(labels), torch.LongTensor(center_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Subgraphs_test(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node, k_shot, n_qry):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "        self.k_shot = k_shot\n",
    "        self.n_qry = n_qry\n",
    "        \n",
    "        self.labels = np.unique(self.data.label.values)\n",
    "        self.labels_dict = dict(zip(list(self.labels), list(range(len(self.labels)))))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        support = []\n",
    "        query = []\n",
    "        \n",
    "        for i in self.labels:\n",
    "            df_labels = self.data[self.data.label == i].reset_index(drop = True)\n",
    "            support = support + list(df_labels.sample(n = self.k_shot)['name'].values)\n",
    "            query = query + list(df_labels[~df_labels.name.isin(support)].sample(n = self.n_qry)['name'].values)\n",
    "        \n",
    "        return [self.subgraph_list[i] for i in support], [self.labels_dict[self.subgraph2label[i]] for i in support], [self.subgraph2center_node[i] for i in support], [self.subgraph_list[i] for i in query], [self.labels_dict[self.subgraph2label[i]] for i in query], [self.subgraph2center_node[i] for i in query]\n",
    "\n",
    "    def __len__(self):\n",
    "        # 100 \"batches\"\n",
    "        return 100\n",
    "    \n",
    "def collate_test(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    \n",
    "    graphs, labels, center_nodes, g_qry, l_qry, c_qry = map(list, zip(*samples))\n",
    "    batched_graph_spt = dgl.batch(graphs[0])\n",
    "    batched_graph_qry = dgl.batch(g_qry[0])\n",
    "    \n",
    "    return batched_graph_spt, torch.LongTensor(labels), torch.LongTensor(center_nodes), batched_graph_qry, torch.LongTensor(l_qry), torch.LongTensor(c_qry)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, g, to_fetch, features):\n",
    "        # For undirected graphs, in_degree is the same as\n",
    "        # out_degree.\n",
    "        #h = g.in_degrees().view(-1, 1).float().to(device)\n",
    "        #h = torch.tensor([1.]*g.number_of_nodes()).view(-1, 1).float()\n",
    "        #h = torch.tensor(features)\n",
    "        h = features.float()\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "        g.ndata['h'] = h\n",
    "        #print(h.shape)\n",
    "        #hg = dgl.mean_nodes(g, 'h')\n",
    "        #print(to_fetch)\n",
    "        num_nodes_ = g.batch_num_nodes\n",
    "        num_nodes_.insert(0, 0)\n",
    "        offset = torch.cumsum(torch.LongTensor(num_nodes_), dim = 0)[:-1].to(device)\n",
    "        hg = h[to_fetch + offset]\n",
    "        #print(hg.shape)\n",
    "        #print(hg.shape)\n",
    "        #print(h[0].shape)\n",
    "        #hg = h[g.nodes[0].data['center_node'].detach().numpy()[0]]\n",
    "        #print(hg.shape)\n",
    "        return hg, self.classify(hg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "import pickle\n",
    "path = './data/cora/'\n",
    "with open(path + 'list_subgraph.pkl', 'rb') as f:\n",
    "    total_subgraph = pickle.load(f)\n",
    "    \n",
    "with open(path + 'label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "with open(path + 'center.pkl', 'rb') as f:\n",
    "    center_node = pickle.load(f)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier_Finetune(nn.Module):\n",
    "    def __init__(self, hidden_dim, n_classes):\n",
    "        super(Classifier_Finetune, self).__init__()\n",
    "\n",
    "        self.classify = nn.Linear(hidden_dim, n_classes)\n",
    "\n",
    "    def forward(self, model, g, to_fetch, features):\n",
    "        \n",
    "        h, _ = model(g, to_fetch, features)\n",
    "        \n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(fold_n):\n",
    "    path = './data/cora/'\n",
    "\n",
    "    path = path + 'fold'+str(fold_n)+'/'\n",
    "    trainset = Subgraphs(path, 'train', total_subgraph, info, center_node)\n",
    "    valset = Subgraphs(path, 'val', total_subgraph, info, center_node)\n",
    "    testset = Subgraphs_test(path, 'test', total_subgraph, info, center_node, 1, 12)\n",
    "\n",
    "    data_loader_test = DataLoader(testset, batch_size=1, shuffle=True, collate_fn=collate_test)\n",
    "\n",
    "    data_loader = DataLoader(trainset, batch_size=64, shuffle=True, collate_fn=collate)\n",
    "    \n",
    "    model = Classifier(1433, 256, 7)\n",
    "    model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "    model.train()\n",
    "    \n",
    "    print('model training')\n",
    "    for epoch in range(10):\n",
    "        epoch_loss = 0\n",
    "        for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "            bg = bg.to(device)\n",
    "            label = label.to(device)\n",
    "            features = bg.ndata['h']\n",
    "            hid, prediction = model(bg, to_fetch, features)\n",
    "            #print(prediction.shape)\n",
    "            loss = loss_func(prediction, label)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.detach().item()\n",
    "        epoch_loss /= (iter + 1)\n",
    "        \n",
    "        if epoch%1 ==0:\n",
    "            print('Epoch {}, training loss {:.4f}'.format(epoch, epoch_loss))\n",
    "    \n",
    "    print('model finetuning')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        model_finetune = Classifier_Finetune(256, 2)\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "\n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,)    \n",
    "\n",
    "        update_step = 10\n",
    "        loss_steps = []\n",
    "        acc_steps = []\n",
    "\n",
    "        for i in range(update_step):\n",
    "            model_finetune.train()\n",
    "            bg_ = copy.deepcopy(bg)\n",
    "            features = bg_.ndata['h']\n",
    "            prediction = model_finetune(model, bg_, to_fetch, features)\n",
    "            loss = loss_func(prediction, label)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            model_finetune.eval()\n",
    "            bg_ = copy.deepcopy(bg_qry)\n",
    "            features = bg_.ndata['h']\n",
    "            pred_qry = model_finetune(model, bg_, to_fetch_qry, features)\n",
    "            probs_Y = torch.softmax(pred_qry, 1)\n",
    "            argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "\n",
    "            label_q = torch.tensor(label_qry).float().view(-1, 1)\n",
    "            acc = (label_q == argmax_Y.detach().cpu().float()).sum().item() / len(label_q) * 100\n",
    "\n",
    "            acc_steps.append(acc)\n",
    "            loss_steps.append(loss.item())\n",
    "        accs.append(acc_steps)\n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "    print(np.mean(np.array(accs), 0))\n",
    "    \n",
    "    \n",
    "    print('model KNN')\n",
    "    accs = []\n",
    "    for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_test):\n",
    "        to_fetch = torch.LongTensor(to_fetch.reshape(-1,))\n",
    "        label = label.reshape(-1,)\n",
    "        \n",
    "        to_fetch_qry = torch.LongTensor(to_fetch_qry.reshape(-1,))\n",
    "        label_qry = label_qry.reshape(-1,) \n",
    "        \n",
    "        bg_ = copy.deepcopy(bg)\n",
    "        features = bg_.ndata['h']\n",
    "        h, prediction = model(bg_, to_fetch, features)\n",
    "        \n",
    "        bg_ = copy.deepcopy(bg_qry)\n",
    "        features = bg_.ndata['h']\n",
    "        h_qry, pred_qry = model(bg_, to_fetch_qry, features)\n",
    "        \n",
    "        from sklearn.metrics import pairwise_distances\n",
    "        \n",
    "        dist = pairwise_distances(h.detach().numpy(), h_qry.detach().numpy())\n",
    "        \n",
    "        y_pred = np.argmin(dist, 0).reshape(-1,)\n",
    "        \n",
    "        label_dict = dict(zip(label.numpy(), list(range(label.numpy().reshape(-1,).shape[0]))))\n",
    "        y_pred = np.array([label_dict[i] for i in y_pred])\n",
    "        #print(y_pred.shape)\n",
    "        label_q = label_qry.float().view(-1, ).numpy()\n",
    "\n",
    "        acc = sum(label_q == y_pred) / len(label_q) * 100\n",
    "\n",
    "        accs.append(acc)\n",
    "        \n",
    "        if iter%20 == 0:\n",
    "            print('step ', iter)\n",
    "            \n",
    "    print(np.mean(np.array(accs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model training\n",
      "Epoch 0, training loss 1.2025\n",
      "Epoch 1, training loss 0.7275\n",
      "Epoch 2, training loss 0.4294\n",
      "Epoch 3, training loss 0.2130\n",
      "Epoch 4, training loss 0.0939\n",
      "Epoch 5, training loss 0.0591\n",
      "Epoch 6, training loss 0.0457\n",
      "Epoch 7, training loss 0.0470\n",
      "Epoch 8, training loss 0.0324\n",
      "Epoch 9, training loss 0.0296\n",
      "model finetuning\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kexinhuang/torch-env/lib/python3.7/site-packages/ipykernel_launcher.py:70: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "[51.25       51.79166667 54.25       56.58333333 60.33333333 64.20833333\n",
      " 67.04166667 70.25       75.04166667 78.625     ]\n",
      "model KNN\n",
      "step  0\n",
      "step  20\n",
      "step  40\n",
      "step  60\n",
      "step  80\n",
      "90.37500000000001\n"
     ]
    }
   ],
   "source": [
    "run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
