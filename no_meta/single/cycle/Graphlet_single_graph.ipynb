{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import collections\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "from torch.utils.data import DataLoader\n",
    "import dgl\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.special import comb\n",
    "from itertools import combinations \n",
    "import networkx.algorithms.isomorphism as iso\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "\n",
    "import copy\n",
    "\n",
    "class Subgraphs(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph_list, subgraph2label, subgraph2center_node):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        self.subgraph_list = subgraph_list\n",
    "        self.subgraph2center_node = subgraph2center_node\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        return self.subgraph_list[self.data.iloc[index]['name']], self.subgraph2label[self.data.iloc[index]['name']], self.subgraph2center_node[self.data.iloc[index]['name']] \n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    graphs, labels, center_nodes = map(list, zip(*samples))\n",
    "    batched_graph = dgl.batch(graphs)\n",
    "    return batched_graph, torch.LongTensor(labels), torch.LongTensor(center_nodes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_graphlet(n):\n",
    "    non_iso_graph = []\n",
    "    non_iso_graph_adj = []\n",
    "    dgl_graph = []\n",
    "    for i in tqdm(range(n-1, int(comb(n, 2))+1)):\n",
    "    # for each of these possible # of edges\n",
    "        arr = np.array(range(int((n**2-n)/2)))\n",
    "        all_comb = list(combinations(arr, i)) \n",
    "        # all possible combination of edge positions \n",
    "        indices = np.triu_indices(n, 1)\n",
    "        for m in range(len(all_comb)):\n",
    "            # iterate over all these graphs\n",
    "            adj = np.zeros((n,n))\n",
    "            adj[indices[0][np.array(all_comb[m])], indices[1][np.array(all_comb[m])]] = 1\n",
    "            adj_temp = adj\n",
    "            adj = adj + adj.T\n",
    "            #print(adj)\n",
    "            if sum(np.sum(adj_temp, axis = 0) == 0) == 1:\n",
    "                #the graph has to be connected\n",
    "                new_graph = nx.from_numpy_matrix(adj)\n",
    "                if len(non_iso_graph) == 0:\n",
    "                    non_iso_graph.append(new_graph)\n",
    "                    non_iso_graph_adj.append(adj)\n",
    "                    S = dgl.DGLGraph()\n",
    "                    S.from_networkx(new_graph)\n",
    "                    dgl_graph.append(S)\n",
    "                else:\n",
    "                    is_iso = False\n",
    "                    for g in non_iso_graph:\n",
    "                        if iso.is_isomorphic(g, new_graph):\n",
    "                            #print('yes')\n",
    "                            is_iso = True\n",
    "                            break\n",
    "                    if not is_iso:\n",
    "                        # not isomorphic to any of the current graphs\n",
    "                        non_iso_graph.append(new_graph)\n",
    "                        non_iso_graph_adj.append(adj)\n",
    "                        \n",
    "                        S = dgl.DGLGraph()\n",
    "                        S.from_networkx(new_graph)\n",
    "                        dgl_graph.append(S)\n",
    "                        \n",
    "    \n",
    "    print('There are {} non-isomorphic graphs'.format(len(non_iso_graph)))\n",
    "    return dgl_graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl.function as fn\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "# Sends a message of node feature h.\n",
    "msg = fn.copy_src(src='h', out='m')\n",
    "\n",
    "def reduce(nodes):\n",
    "    \"\"\"Take an average over all neighbor node features hu and use it to\n",
    "    overwrite the original node feature.\"\"\"\n",
    "    accum = torch.mean(nodes.mailbox['m'], 1)\n",
    "    return {'h': accum}\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    \"\"\"Update the node feature hv with ReLU(Whv+b).\"\"\"\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        # Initialize the node features with h.\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(msg, reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_dim, n_classes, attention_dim):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            GCN(in_dim, hidden_dim, F.relu),\n",
    "            GCN(hidden_dim, hidden_dim, F.relu)])\n",
    "        self.classify = nn.Linear(hidden_dim + attention_dim, n_classes)\n",
    "        \n",
    "        self.query = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.key = nn.Linear(hidden_dim, attention_dim)\n",
    "        self.value = nn.Linear(30, attention_dim)\n",
    "            \n",
    "    def forward(self, g, to_fetch, features, graphlets):\n",
    "        h = g.in_degrees().view(-1, 1).float().to(device)\n",
    "        h_graphlets = graphlets.in_degrees().view(-1, 1).float().to(device)\n",
    "\n",
    "        #h = torch.tensor([1.]*g.number_of_nodes()).view(-1, 1).float()\n",
    "\n",
    "        for conv in self.layers:\n",
    "            h = conv(g, h)\n",
    "            h_graphlets = conv(graphlets, h_graphlets)\n",
    "\n",
    "        g.ndata['h'] = h\n",
    "        graphlets.ndata['h'] = h_graphlets\n",
    "\n",
    "        num_nodes_ = g.batch_num_nodes\n",
    "        num_nodes_.insert(0, 0)\n",
    "        offset = torch.cumsum(torch.LongTensor(num_nodes_), dim = 0)[:-1].to(device)\n",
    "        hg = h[to_fetch + offset]\n",
    "        \n",
    "        h_graphlets = dgl.mean_nodes(graphlets, 'h')\n",
    "        \n",
    "        Q = self.query(hg)\n",
    "        K = self.key(h_graphlets)\n",
    "        attention_scores = torch.matmul(Q, K.T)\n",
    "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
    "        context = self.value(attention_probs)\n",
    "        \n",
    "        h = torch.cat((context, hg), 1)\n",
    "        \n",
    "        return self.classify(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 923.45it/s]\n",
      "100%|██████████| 2/2 [00:00<00:00, 934.04it/s]\n",
      "100%|██████████| 4/4 [00:00<00:00, 454.33it/s]\n",
      "100%|██████████| 7/7 [00:00<00:00, 37.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 non-isomorphic graphs\n",
      "There are 2 non-isomorphic graphs\n",
      "There are 6 non-isomorphic graphs\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# create graphlets\n",
    "graphs = []\n",
    "for i in range(1, 5):\n",
    "        graphs = graphs + generate_graphlet(i+1)\n",
    "\n",
    "print('There are {} number of graphlets'.format(len(graphs)))\n",
    "graphlets = dgl.batch(graphs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "path = '../../../data/single_graph/cycle/'\n",
    "with open(path + 'list_subgraph.pkl', 'rb') as f:\n",
    "    total_subgraph = pickle.load(f)\n",
    "    \n",
    "with open(path + 'label.pkl', 'rb') as f:\n",
    "    info = pickle.load(f)\n",
    "\n",
    "with open(path + 'center.pkl', 'rb') as f:\n",
    "    center_node = pickle.load(f)\n",
    "    \n",
    "path = path + 'fold1/'\n",
    "trainset = Subgraphs(path, 'train', total_subgraph, info, center_node)\n",
    "valset = Subgraphs(path, 'val', total_subgraph, info, center_node)\n",
    "testset = Subgraphs(path, 'test', total_subgraph, info, center_node)\n",
    "    \n",
    "data_loader = DataLoader(trainset, batch_size=64, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "data_loader_val = DataLoader(valset, batch_size=64, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "data_loader_test = DataLoader(testset, batch_size=64, shuffle=True,\n",
    "                         collate_fn=collate)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss 2.7893 and validation accuracy 3.9216%\n",
      "Epoch 50, training loss 1.8460 and validation accuracy 33.3333%\n",
      "Epoch 100, training loss 1.5739 and validation accuracy 35.2941%\n",
      "Epoch 150, training loss 1.4244 and validation accuracy 39.2157%\n",
      "Epoch 200, training loss 1.3013 and validation accuracy 43.1373%\n",
      "Epoch 250, training loss 1.1998 and validation accuracy 49.0196%\n",
      "Epoch 300, training loss 1.0994 and validation accuracy 47.0588%\n",
      "Epoch 350, training loss 1.0060 and validation accuracy 49.0196%\n",
      "Epoch 400, training loss 0.9191 and validation accuracy 50.9804%\n",
      "Epoch 450, training loss 0.8389 and validation accuracy 45.0980%\n",
      "Accuracy of argmax predictions on the test set: 54.411765%\n"
     ]
    }
   ],
   "source": [
    "def test(data_loader, model):\n",
    "    model.eval()\n",
    "    y_pred = []\n",
    "    y_label = []\n",
    "    epoch_loss = 0\n",
    "    for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "        features = bg.ndata['h']\n",
    "        prediction = model(bg, to_fetch, features, graphlets)\n",
    "        loss_func = nn.CrossEntropyLoss()\n",
    "        loss = loss_func(prediction, label)\n",
    "        \n",
    "        probs_Y = torch.softmax(prediction, 1)\n",
    "        argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "        \n",
    "        y_pred = y_pred + argmax_Y.detach().numpy().flatten().tolist()\n",
    "        y_label = y_label + label.numpy().flatten().tolist()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)    \n",
    "    acc = (np.array(y_label) == np.array(y_pred)).sum().item() / len(test_Y) * 100\n",
    "    return acc, epoch_loss\n",
    "\n",
    "def test2(valset, model):\n",
    "    \n",
    "    model.eval()\n",
    "    # Convert a list of tuples to two lists\n",
    "    test_X, test_Y, center_nodes = map(list, zip(*valset))\n",
    "    test_bg = dgl.batch(test_X)\n",
    "    test_Y = torch.tensor(test_Y).float().view(-1, 1)\n",
    "    test_bg.to(device)\n",
    "    test_Y.to(device)\n",
    "    center_nodes = torch.LongTensor(center_nodes).to(device)\n",
    "\n",
    "    probs_Y = torch.softmax(model(test_bg, center_nodes, test_bg.ndata['h'], graphlets), 1)\n",
    "    argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "    res = (test_Y == argmax_Y.detach().cpu().float()).sum().item() / len(test_Y) * 100\n",
    "    #print('Accuracy of argmax predictions on the validation set: {:4f}%'.format(res)\n",
    "    return res\n",
    "    \n",
    "    \n",
    "# Create model\n",
    "model = Classifier(1, 128, max(info.values()) + 1, 32)\n",
    "model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "model.train()\n",
    "\n",
    "max_res = 0\n",
    "model_max = copy.deepcopy(model)\n",
    "\n",
    "epoch_losses = []\n",
    "for epoch in range(500):\n",
    "    epoch_loss = 0\n",
    "    for iter, (bg, label, to_fetch) in enumerate(data_loader):\n",
    "        bg = bg.to(device)\n",
    "        label = label.to(device)\n",
    "        features = bg.ndata['h']\n",
    "        prediction = model(bg, to_fetch, features, graphlets)\n",
    "        #print(prediction.shape)\n",
    "        loss = loss_func(prediction, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.detach().item()\n",
    "    epoch_loss /= (iter + 1)\n",
    "    if epoch % 50 == 0:\n",
    "        res = test2(valset, model)\n",
    "        if res > max_res:\n",
    "            model_max = copy.deepcopy(model)\n",
    "        print('Epoch {}, training loss {:.4f} and validation accuracy {:.4f}%'.format(epoch, epoch_loss, res))\n",
    "        #print('Accuracy of argmax predictions on the val set: {:4f}%'.format(res))\n",
    "    epoch_losses.append(epoch_loss)\n",
    "\n",
    "res = test2(testset, model_max)\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(res))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of argmax predictions on the test set: 54.411765%\n"
     ]
    }
   ],
   "source": [
    "model = model_max\n",
    "valset = Subgraphs(path, 'test', total_subgraph, info, center_node)\n",
    "#testset = Subgraphs(path, mode='test', path_s = 'list_subgraph.pkl', path_l = 'label.pkl')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "model.eval()\n",
    "# Convert a list of tuples to two lists\n",
    "test_X, test_Y, center_nodes = map(list, zip(*valset))\n",
    "test_bg = dgl.batch(test_X)\n",
    "test_Y = torch.tensor(test_Y).float().view(-1, 1)\n",
    "test_bg.to(device)\n",
    "test_Y.to(device)\n",
    "center_nodes = torch.LongTensor(center_nodes).to(device)\n",
    "\n",
    "probs_Y = torch.softmax(model(test_bg, center_nodes, test_bg.ndata['h'], graphlets), 1)\n",
    "argmax_Y = torch.max(probs_Y, 1)[1].view(-1, 1)\n",
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_Y == argmax_Y.detach().cpu().float()).sum().item() / len(test_Y) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(model.state_dict(), 'model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = Classifier(1, 256, 10, 16)\n",
    "#model.load_state_dict(torch.load('model.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
