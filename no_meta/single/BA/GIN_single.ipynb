{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "%matplotlib inline\n",
    "import networkx as nx \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "adj = np.load('../../../data/single_graph/BA/graph_adj.npy')\n",
    "rows, cols = np.where(adj == 1)\n",
    "edges = zip(rows.tolist(), cols.tolist())\n",
    "G = nx.Graph()\n",
    "G.add_edges_from(edges)\n",
    "labels = pd.read_csv('../../../data/single_graph/BA/data.csv').label.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G.number_of_nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch.conv import GINConv\n",
    "from dgl.nn.pytorch.glob import SumPooling, AvgPooling, MaxPooling\n",
    "\n",
    "\n",
    "class ApplyNodeFunc(nn.Module):\n",
    "    \"\"\"Update the node feature hv with MLP, BN and ReLU.\"\"\"\n",
    "    def __init__(self, mlp):\n",
    "        super(ApplyNodeFunc, self).__init__()\n",
    "        self.mlp = mlp\n",
    "        self.bn = nn.BatchNorm1d(self.mlp.output_dim)\n",
    "\n",
    "    def forward(self, h):\n",
    "        h = self.mlp(h)\n",
    "        h = self.bn(h)\n",
    "        h = F.relu(h)\n",
    "        return h\n",
    "\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    \"\"\"MLP with linear output\"\"\"\n",
    "    def __init__(self, num_layers, input_dim, hidden_dim, output_dim):\n",
    "        \"\"\"MLP layers construction\n",
    "        Paramters\n",
    "        ---------\n",
    "        num_layers: int\n",
    "            The number of linear layers\n",
    "        input_dim: int\n",
    "            The dimensionality of input features\n",
    "        hidden_dim: int\n",
    "            The dimensionality of hidden units at ALL layers\n",
    "        output_dim: int\n",
    "            The number of classes for prediction\n",
    "        \"\"\"\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear_or_not = True  # default is linear model\n",
    "        self.num_layers = num_layers\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        if num_layers < 1:\n",
    "            raise ValueError(\"number of layers should be positive!\")\n",
    "        elif num_layers == 1:\n",
    "            # Linear model\n",
    "            self.linear = nn.Linear(input_dim, output_dim)\n",
    "        else:\n",
    "            # Multi-layer model\n",
    "            self.linear_or_not = False\n",
    "            self.linears = torch.nn.ModuleList()\n",
    "            self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "            self.linears.append(nn.Linear(input_dim, hidden_dim))\n",
    "            for layer in range(num_layers - 2):\n",
    "                self.linears.append(nn.Linear(hidden_dim, hidden_dim))\n",
    "            self.linears.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "            for layer in range(num_layers - 1):\n",
    "                self.batch_norms.append(nn.BatchNorm1d((hidden_dim)))\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.linear_or_not:\n",
    "            # If linear model\n",
    "            return self.linear(x)\n",
    "        else:\n",
    "            # If MLP\n",
    "            h = x\n",
    "            for i in range(self.num_layers - 1):\n",
    "                h = F.relu(self.batch_norms[i](self.linears[i](h)))\n",
    "            return self.linears[-1](h)\n",
    "\n",
    "\n",
    "class GIN(nn.Module):\n",
    "    \"\"\"GIN model\"\"\"\n",
    "    def __init__(self, num_layers, num_mlp_layers, input_dim, hidden_dim,\n",
    "                 output_dim, final_dropout, learn_eps, graph_pooling_type,\n",
    "                 neighbor_pooling_type):\n",
    "        \"\"\"model parameters setting\n",
    "        Paramters\n",
    "        ---------\n",
    "        num_layers: int\n",
    "            The number of linear layers in the neural network\n",
    "        num_mlp_layers: int\n",
    "            The number of linear layers in mlps\n",
    "        input_dim: int\n",
    "            The dimensionality of input features\n",
    "        hidden_dim: int\n",
    "            The dimensionality of hidden units at ALL layers\n",
    "        output_dim: int\n",
    "            The number of classes for prediction\n",
    "        final_dropout: float\n",
    "            dropout ratio on the final linear layer\n",
    "        learn_eps: boolean\n",
    "            If True, learn epsilon to distinguish center nodes from neighbors\n",
    "            If False, aggregate neighbors and center nodes altogether.\n",
    "        neighbor_pooling_type: str\n",
    "            how to aggregate neighbors (sum, mean, or max)\n",
    "        graph_pooling_type: str\n",
    "            how to aggregate entire nodes in a graph (sum, mean or max)\n",
    "        \"\"\"\n",
    "        super(GIN, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.learn_eps = learn_eps\n",
    "\n",
    "        # List of MLPs\n",
    "        self.ginlayers = torch.nn.ModuleList()\n",
    "        self.batch_norms = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(self.num_layers - 1):\n",
    "            if layer == 0:\n",
    "                mlp = MLP(num_mlp_layers, input_dim, hidden_dim, hidden_dim)\n",
    "            else:\n",
    "                mlp = MLP(num_mlp_layers, hidden_dim, hidden_dim, hidden_dim)\n",
    "\n",
    "            self.ginlayers.append(\n",
    "                GINConv(ApplyNodeFunc(mlp), neighbor_pooling_type, 0, self.learn_eps))\n",
    "            self.batch_norms.append(nn.BatchNorm1d(hidden_dim))\n",
    "\n",
    "        # Linear function for graph poolings of output of each layer\n",
    "        # which maps the output of different layers into a prediction score\n",
    "        self.linears_prediction = torch.nn.ModuleList()\n",
    "\n",
    "        for layer in range(num_layers):\n",
    "            if layer == 0:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(input_dim, output_dim))\n",
    "            else:\n",
    "                self.linears_prediction.append(\n",
    "                    nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "        self.drop = nn.Dropout(final_dropout)\n",
    "\n",
    "        if graph_pooling_type == 'sum':\n",
    "            self.pool = SumPooling()\n",
    "        elif graph_pooling_type == 'mean':\n",
    "            self.pool = AvgPooling()\n",
    "        elif graph_pooling_type == 'max':\n",
    "            self.pool = MaxPooling()\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "    def forward(self, g, h):\n",
    "        # list of hidden representation at each layer (including input)\n",
    "        #hidden_rep = [h]\n",
    "        h = g.in_degrees().view(-1, 1).float()\n",
    "        #h = torch.tensor([1.]*g.number_of_nodes()).view(-1, 1).float()\n",
    "        \n",
    "        for i in range(self.num_layers - 1):\n",
    "            h = self.ginlayers[i](g, h)\n",
    "            h = self.batch_norms[i](h)\n",
    "            h = F.relu(h)\n",
    "            #hidden_rep.append(h)\n",
    "\n",
    "        #score_over_layer = 0\n",
    "\n",
    "        # perform pooling over all nodes in each graph in every layer\n",
    "        #for i, h in enumerate(hidden_rep):\n",
    "        #    pooled_h = self.pool(g, h)\n",
    "        #    score_over_layer += self.drop(self.linears_prediction[i](pooled_h))\n",
    "\n",
    "        return h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "labels = torch.tensor(labels)\n",
    "inputs = torch.eye(G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = dgl.DGLGraph()\n",
    "S.from_networkx(G)\n",
    "S.ndata['h'] = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([200, 200])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "labeled_nodes = np.random.choice(list(range(G.number_of_nodes())), int(G.number_of_nodes() * 0.15), replace = False)\n",
    "labels_train = labels[labeled_nodes]\n",
    "\n",
    "unlabelled_nodes = [i for i in list(range(G.number_of_nodes())) if i not in labeled_nodes]\n",
    "val_nodes = np.random.choice(unlabelled_nodes, int(len(unlabelled_nodes)*0.2), replace = False)\n",
    "test_nodes = [i for i in unlabelled_nodes if i not in val_nodes]\n",
    "\n",
    "val_label = labels[val_nodes]\n",
    "test_label = labels[test_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 58,  40,  34, 102, 184, 198,  95,   4,  29, 168, 171,  18,  11,\n",
       "        89, 110, 118, 159,  35, 136,  59,  51,  16,  44,  94,  31, 162,\n",
       "        38,  28, 193,  27])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeled_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 3.6014\n",
      "Epoch 100 | Loss: 0.2684\n",
      "Epoch 200 | Loss: 0.2099\n",
      "Epoch 300 | Loss: 0.1861\n",
      "Epoch 400 | Loss: 0.1755\n",
      "Epoch 500 | Loss: 0.1648\n",
      "Epoch 600 | Loss: 0.1624\n",
      "Epoch 700 | Loss: 0.1615\n",
      "Epoch 800 | Loss: 0.1612\n",
      "Epoch 900 | Loss: 0.1611\n",
      "Epoch 1000 | Loss: 0.1610\n",
      "Epoch 1100 | Loss: 0.1609\n",
      "Epoch 1200 | Loss: 0.1609\n",
      "Epoch 1300 | Loss: 0.1609\n",
      "Epoch 1400 | Loss: 0.1608\n",
      "Epoch 1500 | Loss: 0.1608\n",
      "Epoch 1600 | Loss: 0.1608\n",
      "Epoch 1700 | Loss: 0.1608\n",
      "Epoch 1800 | Loss: 0.1608\n",
      "Epoch 1900 | Loss: 0.1608\n"
     ]
    }
   ],
   "source": [
    "net = GIN(3, 2,\n",
    "        1, 32, 14,\n",
    "        0.5, True,\n",
    "        \"max\", \"sum\")\n",
    "\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "all_logits = []\n",
    "for epoch in range(2000):\n",
    "    logits = net(S, inputs)\n",
    "    # we save the logits for visualization later\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    # we only compute loss for labeled nodes\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch%100 == 0:\n",
    "        print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "logits = net(S, inputs)\n",
    "# we save the logits for visualization later\n",
    "logp = F.log_softmax(logits, 1)\n",
    "# we only compute loss for labeled nodes\n",
    "#loss = F.nll_loss(logp[labeled_nodes], labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_Y = torch.max(logp[test_nodes], 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of argmax predictions on the test set: 27.941176%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_label == argmax_Y.float()).sum().item() / len(test_label) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
