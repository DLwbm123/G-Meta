{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "%matplotlib inline\n",
    "import networkx as nx \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "os.chdir('../graphwave/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import graphwave\n",
    "from graphwave.shapes import build_graph\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class graph_data(Dataset):\n",
    "\n",
    "\n",
    "    def __init__(self, root, mode, subgraph2label, k_shot, n_qry, batchsz = 100):   \n",
    "\n",
    "        self.subgraph2label = subgraph2label\n",
    "        \n",
    "        self.data = pd.read_csv(os.path.join(root, mode + '.csv'))  # csv path\n",
    "        self.k_shot = k_shot\n",
    "        self.n_qry = n_qry\n",
    "        self.batchsz = batchsz\n",
    "        \n",
    "        self.labels = np.unique(self.data.label.values)\n",
    "        self.labels_dict = dict(zip(list(self.labels), list(range(len(self.labels)))))\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        support = []\n",
    "        query = []\n",
    "        \n",
    "        for i in self.labels:\n",
    "            df_labels = self.data[self.data.label == i].reset_index(drop = True)\n",
    "            support = support + list(df_labels.sample(n = self.k_shot)['name'].values)\n",
    "            query = query + list(df_labels[~df_labels.name.isin(support)].sample(n = self.n_qry)['name'].values)\n",
    "            \n",
    "        return support, [self.labels_dict[self.subgraph2label[i]] for i in support], query, [self.labels_dict[self.subgraph2label[i]] for i in query]\n",
    "\n",
    "    def __len__(self):\n",
    "        # as we have built up to batchsz of sets, you can sample some small batch size of sets.\n",
    "        return self.batchsz\n",
    "    \n",
    "def collate(samples):\n",
    "    # The input `samples` is a list of pairs\n",
    "    #  (graph, label).\n",
    "    \n",
    "    spt_idx, l_spt, qry_idx, l_qry = map(list, zip(*samples))\n",
    "    return torch.LongTensor(spt_idx), torch.LongTensor(l_spt), torch.LongTensor(qry_idx), torch.LongTensor(l_qry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the message and reduce function\n",
    "# NOTE: We ignore the GCN's normalization constant c_ij for this tutorial.\n",
    "def gcn_message(edges):\n",
    "    # The argument is a batch of edges.\n",
    "    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n",
    "    return {'msg' : edges.src['h']}\n",
    "\n",
    "def gcn_reduce(nodes):\n",
    "    # The argument is a batch of nodes.\n",
    "    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n",
    "    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}\n",
    "\n",
    "# Define the GCNLayer module\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first set the node features\n",
    "        g.ndata['h'] = inputs\n",
    "        # trigger message passing on all edges\n",
    "        g.send(g.edges(), gcn_message)\n",
    "        # trigger aggregation at all nodes\n",
    "        g.recv(g.nodes(), gcn_reduce)\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        # perform linear transformation\n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (gcn1): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (gcn2): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, in_feats, hid_dim, out_feats):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(in_feats, hid_dim, F.relu)\n",
    "        self.gcn2 = GCN(hid_dim, out_feats, None)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# multiple graph setting \n",
    "\n",
    "task_num = 4\n",
    "k_shot = 1\n",
    "n_qry = 6\n",
    "\n",
    "path = '../data/multiple_graph/BA/META_LABEL/'\n",
    "fold_n = 1\n",
    "adjs = np.load(path + 'graphs_adj.npy', allow_pickle=True)\n",
    "dgl_Gs = []\n",
    "\n",
    "for i in range(adjs.shape[0]):\n",
    "    adj = adjs[i]\n",
    "    G = nx.from_numpy_matrix(adj)\n",
    "    S = dgl.DGLGraph()\n",
    "    S.from_networkx(G)\n",
    "    dgl_Gs.append(S)\n",
    "    \n",
    "    \n",
    "path = path + 'fold'+str(fold_n)+'/'\n",
    "trainset = graph_data(path, 'train', info, k_shot, n_qry, 1000)\n",
    "valset = graph_data(path, 'val', info, k_shot, n_qry, 100)\n",
    "testset = graph_data(path, 'test', info, k_shot, n_qry, 100)\n",
    "\n",
    "data_loader_test = DataLoader(testset, batch_size=1, shuffle=True, collate_fn=collate)\n",
    "data_loader_val = DataLoader(valset, batch_size=1, shuffle=True, collate_fn=collate)\n",
    "data_loader_train = DataLoader(trainset, batch_size=task_num, shuffle=True, collate_fn=collate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.3168\n",
      "Epoch 1 | Loss: 2.1854\n",
      "Epoch 2 | Loss: 2.0515\n",
      "Epoch 3 | Loss: 1.9082\n",
      "Epoch 4 | Loss: 1.7647\n",
      "Epoch 5 | Loss: 1.6287\n",
      "Epoch 6 | Loss: 1.4966\n",
      "Epoch 7 | Loss: 1.3618\n",
      "Epoch 8 | Loss: 1.2255\n",
      "Epoch 9 | Loss: 1.0934\n",
      "Epoch 10 | Loss: 0.9700\n",
      "Epoch 11 | Loss: 0.8562\n",
      "Epoch 12 | Loss: 0.7535\n",
      "Epoch 13 | Loss: 0.6632\n",
      "Epoch 14 | Loss: 0.5845\n",
      "Epoch 15 | Loss: 0.5139\n",
      "Epoch 16 | Loss: 0.4495\n",
      "Epoch 17 | Loss: 0.3933\n",
      "Epoch 18 | Loss: 0.3482\n",
      "Epoch 19 | Loss: 0.3133\n",
      "Epoch 20 | Loss: 0.2853\n",
      "Epoch 21 | Loss: 0.2632\n",
      "Epoch 22 | Loss: 0.2456\n",
      "Epoch 23 | Loss: 0.2296\n",
      "Epoch 24 | Loss: 0.2139\n",
      "Epoch 25 | Loss: 0.1989\n",
      "Epoch 26 | Loss: 0.1850\n",
      "Epoch 27 | Loss: 0.1718\n",
      "Epoch 28 | Loss: 0.1596\n",
      "Epoch 29 | Loss: 0.1497\n",
      "Epoch 30 | Loss: 0.1424\n",
      "Epoch 31 | Loss: 0.1371\n",
      "Epoch 32 | Loss: 0.1324\n",
      "Epoch 33 | Loss: 0.1274\n",
      "Epoch 34 | Loss: 0.1218\n",
      "Epoch 35 | Loss: 0.1162\n",
      "Epoch 36 | Loss: 0.1114\n",
      "Epoch 37 | Loss: 0.1080\n",
      "Epoch 38 | Loss: 0.1057\n",
      "Epoch 39 | Loss: 0.1037\n",
      "Epoch 40 | Loss: 0.1015\n",
      "Epoch 41 | Loss: 0.0988\n",
      "Epoch 42 | Loss: 0.0958\n",
      "Epoch 43 | Loss: 0.0931\n",
      "Epoch 44 | Loss: 0.0911\n",
      "Epoch 45 | Loss: 0.0897\n",
      "Epoch 46 | Loss: 0.0887\n",
      "Epoch 47 | Loss: 0.0875\n",
      "Epoch 48 | Loss: 0.0859\n",
      "Epoch 49 | Loss: 0.0842\n",
      "Epoch 50 | Loss: 0.0826\n",
      "Epoch 51 | Loss: 0.0814\n",
      "Epoch 52 | Loss: 0.0807\n",
      "Epoch 53 | Loss: 0.0800\n",
      "Epoch 54 | Loss: 0.0793\n",
      "Epoch 55 | Loss: 0.0784\n",
      "Epoch 56 | Loss: 0.0774\n",
      "Epoch 57 | Loss: 0.0765\n",
      "Epoch 58 | Loss: 0.0759\n",
      "Epoch 59 | Loss: 0.0755\n",
      "Epoch 60 | Loss: 0.0751\n",
      "Epoch 61 | Loss: 0.0746\n",
      "Epoch 62 | Loss: 0.0740\n",
      "Epoch 63 | Loss: 0.0734\n",
      "Epoch 64 | Loss: 0.0730\n",
      "Epoch 65 | Loss: 0.0726\n",
      "Epoch 66 | Loss: 0.0723\n",
      "Epoch 67 | Loss: 0.0720\n",
      "Epoch 68 | Loss: 0.0716\n",
      "Epoch 69 | Loss: 0.0712\n",
      "Epoch 70 | Loss: 0.0709\n",
      "Epoch 71 | Loss: 0.0706\n",
      "Epoch 72 | Loss: 0.0704\n",
      "Epoch 73 | Loss: 0.0701\n",
      "Epoch 74 | Loss: 0.0699\n",
      "Epoch 75 | Loss: 0.0696\n",
      "Epoch 76 | Loss: 0.0693\n",
      "Epoch 77 | Loss: 0.0691\n",
      "Epoch 78 | Loss: 0.0689\n",
      "Epoch 79 | Loss: 0.0687\n"
     ]
    }
   ],
   "source": [
    "net = Net(1, 64, np.unique(labels).shape[0])\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "\n",
    "    \n",
    "for iter, (bg, label, to_fetch, bg_qry, label_qry, to_fetch_qry) in enumerate(data_loader_train):\n",
    "    \n",
    "    \n",
    "    for graph_idx, S in enumerate(dgl_Gs):    \n",
    "        features = S.in_degrees().view(-1, 1).float().to(device)    \n",
    "        logits = net(S, features, idx)\n",
    "        \n",
    "        \n",
    "        \n",
    "    # we save the logits for visualization later\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    # we only compute loss for labeled nodes\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "logits = net(S, inputs)\n",
    "# we save the logits for visualization later\n",
    "logp = F.log_softmax(logits, 1)\n",
    "# we only compute loss for labeled nodes\n",
    "#loss = F.nll_loss(logp[labeled_nodes], labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_Y = torch.max(logp[unlabelled_nodes], 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of argmax predictions on the test set: 44.751381%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_label == argmax_Y.float()).sum().item() / len(test_label) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
