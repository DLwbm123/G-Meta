{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dgl\n",
    "%matplotlib inline\n",
    "import networkx as nx \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "import os\n",
    "os.chdir('../graphwave/')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import graphwave\n",
    "from graphwave.shapes import build_graph\n",
    "\n",
    "import dgl.function as fn\n",
    "import torch as th\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl import DGLGraph\n",
    "\n",
    "width_basis = 200\n",
    "\n",
    "### 1. Choose the basis (cycle, torus or chain)\n",
    "basis_type = \"cycle\" \n",
    "\n",
    "### 2. Add the shapes \n",
    "n_shapes = 30  \n",
    "list_shapes = [[\"house\"]] * n_shapes + [[\"fan\"]] * n_shapes + [[\"star\"]] * n_shapes\n",
    "\n",
    "### 3. Pass all these parameters to the Graph Structure\n",
    "add_edges = 0 # random edges to add\n",
    "G, communities, _ , role_id = build_graph.build_structure(width_basis, basis_type, list_shapes, start=0,\n",
    "                                       add_random_edges=add_edges, plot=False,\n",
    "                                       savefig=False)\n",
    "d = dict(zip(np.unique(role_id), range(len(np.unique(role_id)))))\n",
    "labels = np.array([d[i] for i in role_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dgl.data import citation_graph as citegrh\n",
    "import networkx as nx\n",
    "def load_cora_data():\n",
    "    data = citegrh.load_cora()\n",
    "    features = th.FloatTensor(data.features)\n",
    "    labels = th.LongTensor(data.labels)\n",
    "    train_mask = th.BoolTensor(data.train_mask)\n",
    "    test_mask = th.BoolTensor(data.test_mask)\n",
    "    g = data.graph\n",
    "    # add self loop\n",
    "    g.remove_edges_from(nx.selfloop_edges(g))\n",
    "    g = DGLGraph(g)\n",
    "    g.add_edges(g.nodes(), g.nodes())\n",
    "    return g, features, labels, train_mask, test_mask\n",
    "\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 2708 nodes.\n",
      "We have 13264 edges.\n"
     ]
    }
   ],
   "source": [
    "print('We have %d nodes.' % g.number_of_nodes())\n",
    "print('We have %d edges.' % g.number_of_edges())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Define the message and reduce function\n",
    "# NOTE: We ignore the GCN's normalization constant c_ij for this tutorial.\n",
    "def gcn_message(edges):\n",
    "    # The argument is a batch of edges.\n",
    "    # This computes a (batch of) message called 'msg' using the source node's feature 'h'.\n",
    "    return {'msg' : edges.src['h']}\n",
    "\n",
    "def gcn_reduce(nodes):\n",
    "    # The argument is a batch of nodes.\n",
    "    # This computes the new 'h' features by summing received 'msg' in each node's mailbox.\n",
    "    return {'h' : torch.sum(nodes.mailbox['msg'], dim=1)}\n",
    "\n",
    "# Define the GCNLayer module\n",
    "class GCNLayer(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats):\n",
    "        super(GCNLayer, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "\n",
    "    def forward(self, g, inputs):\n",
    "        # g is the graph and the inputs is the input node features\n",
    "        # first set the node features\n",
    "        g.ndata['h'] = inputs\n",
    "        # trigger message passing on all edges\n",
    "        g.send(g.edges(), gcn_message)\n",
    "        # trigger aggregation at all nodes\n",
    "        g.recv(g.nodes(), gcn_reduce)\n",
    "        # get the result node features\n",
    "        h = g.ndata.pop('h')\n",
    "        # perform linear transformation\n",
    "        return self.linear(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn_msg = fn.copy_src(src='h', out='m')\n",
    "gcn_reduce = fn.sum(msg='m', out='h')\n",
    "\n",
    "class NodeApplyModule(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(NodeApplyModule, self).__init__()\n",
    "        self.linear = nn.Linear(in_feats, out_feats)\n",
    "        self.activation = activation\n",
    "\n",
    "    def forward(self, node):\n",
    "        h = self.linear(node.data['h'])\n",
    "        if self.activation is not None:\n",
    "            h = self.activation(h)\n",
    "        return {'h' : h}\n",
    "    \n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, out_feats, activation):\n",
    "        super(GCN, self).__init__()\n",
    "        self.apply_mod = NodeApplyModule(in_feats, out_feats, activation)\n",
    "\n",
    "    def forward(self, g, feature):\n",
    "        g.ndata['h'] = feature\n",
    "        g.update_all(gcn_msg, gcn_reduce)\n",
    "        g.apply_nodes(func=self.apply_mod)\n",
    "        return g.ndata.pop('h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (gcn1): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=1433, out_features=16, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (gcn2): GCN(\n",
      "    (apply_mod): NodeApplyModule(\n",
      "      (linear): Linear(in_features=16, out_features=7, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.gcn1 = GCN(1433, 16, F.relu)\n",
    "        self.gcn2 = GCN(16, 7, None)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        x = self.gcn1(g, features)\n",
    "        x = self.gcn2(g, x)\n",
    "        return x\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kh278/jupyter-env/lib/python3.7/site-packages/ipykernel_launcher.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "labels = torch.tensor(labels)\n",
    "inputs = torch.eye(G.number_of_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, g, features, labels, mask):\n",
    "    model.eval()\n",
    "    with th.no_grad():\n",
    "        logits = model(g, features)\n",
    "        logits = logits[mask]\n",
    "        labels = labels[mask]\n",
    "        _, indices = th.max(logits, dim=1)\n",
    "        correct = th.sum(indices == labels)\n",
    "        return correct.item() * 1.0 / len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = torch.eye(2708)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = [True] * 1 + [False] * (len(train_mask) - 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/n/app/python/3.7.4-ext/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/n/app/python/3.7.4-ext/lib/python3.7/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Loss 2.0012 | Test Acc 0.1390 | Time(s) nan\n",
      "Epoch 00001 | Loss 1.9472 | Test Acc 0.1700 | Time(s) nan\n",
      "Epoch 00002 | Loss 1.8970 | Test Acc 0.2180 | Time(s) nan\n",
      "Epoch 00003 | Loss 1.8499 | Test Acc 0.2390 | Time(s) 0.1600\n",
      "Epoch 00004 | Loss 1.8033 | Test Acc 0.2410 | Time(s) 0.1605\n",
      "Epoch 00005 | Loss 1.7591 | Test Acc 0.2460 | Time(s) 0.1606\n",
      "Epoch 00006 | Loss 1.7159 | Test Acc 0.2530 | Time(s) 0.1606\n",
      "Epoch 00007 | Loss 1.6742 | Test Acc 0.2610 | Time(s) 0.1607\n",
      "Epoch 00008 | Loss 1.6329 | Test Acc 0.2740 | Time(s) 0.1606\n",
      "Epoch 00009 | Loss 1.5912 | Test Acc 0.2830 | Time(s) 0.1608\n",
      "Epoch 00010 | Loss 1.5494 | Test Acc 0.2930 | Time(s) 0.1607\n",
      "Epoch 00011 | Loss 1.5078 | Test Acc 0.2910 | Time(s) 0.1608\n",
      "Epoch 00012 | Loss 1.4664 | Test Acc 0.3060 | Time(s) 0.1608\n",
      "Epoch 00013 | Loss 1.4252 | Test Acc 0.3070 | Time(s) 0.1609\n",
      "Epoch 00014 | Loss 1.3841 | Test Acc 0.3090 | Time(s) 0.1607\n",
      "Epoch 00015 | Loss 1.3435 | Test Acc 0.3090 | Time(s) 0.1608\n",
      "Epoch 00016 | Loss 1.3032 | Test Acc 0.3140 | Time(s) 0.1608\n",
      "Epoch 00017 | Loss 1.2632 | Test Acc 0.3180 | Time(s) 0.1609\n",
      "Epoch 00018 | Loss 1.2237 | Test Acc 0.3180 | Time(s) 0.1609\n",
      "Epoch 00019 | Loss 1.1848 | Test Acc 0.3180 | Time(s) 0.1609\n",
      "Epoch 00020 | Loss 1.1465 | Test Acc 0.3180 | Time(s) 0.1609\n",
      "Epoch 00021 | Loss 1.1089 | Test Acc 0.3180 | Time(s) 0.1610\n",
      "Epoch 00022 | Loss 1.0721 | Test Acc 0.3190 | Time(s) 0.1610\n",
      "Epoch 00023 | Loss 1.0361 | Test Acc 0.3190 | Time(s) 0.1610\n",
      "Epoch 00024 | Loss 1.0010 | Test Acc 0.3200 | Time(s) 0.1610\n",
      "Epoch 00025 | Loss 0.9667 | Test Acc 0.3200 | Time(s) 0.1610\n",
      "Epoch 00026 | Loss 0.9334 | Test Acc 0.3200 | Time(s) 0.1610\n",
      "Epoch 00027 | Loss 0.9009 | Test Acc 0.3200 | Time(s) 0.1610\n",
      "Epoch 00028 | Loss 0.8694 | Test Acc 0.3200 | Time(s) 0.1610\n",
      "Epoch 00029 | Loss 0.8389 | Test Acc 0.3200 | Time(s) 0.1610\n",
      "Epoch 00030 | Loss 0.8092 | Test Acc 0.3210 | Time(s) 0.1610\n",
      "Epoch 00031 | Loss 0.7805 | Test Acc 0.3220 | Time(s) 0.1610\n",
      "Epoch 00032 | Loss 0.7526 | Test Acc 0.3220 | Time(s) 0.1610\n",
      "Epoch 00033 | Loss 0.7257 | Test Acc 0.3220 | Time(s) 0.1610\n",
      "Epoch 00034 | Loss 0.6995 | Test Acc 0.3220 | Time(s) 0.1610\n",
      "Epoch 00035 | Loss 0.6742 | Test Acc 0.3230 | Time(s) 0.1610\n",
      "Epoch 00036 | Loss 0.6500 | Test Acc 0.3260 | Time(s) 0.1610\n",
      "Epoch 00037 | Loss 0.6261 | Test Acc 0.3270 | Time(s) 0.1610\n",
      "Epoch 00038 | Loss 0.6031 | Test Acc 0.3290 | Time(s) 0.1610\n",
      "Epoch 00039 | Loss 0.5809 | Test Acc 0.3310 | Time(s) 0.1610\n",
      "Epoch 00040 | Loss 0.5592 | Test Acc 0.3340 | Time(s) 0.1610\n",
      "Epoch 00041 | Loss 0.5383 | Test Acc 0.3360 | Time(s) 0.1610\n",
      "Epoch 00042 | Loss 0.5179 | Test Acc 0.3350 | Time(s) 0.1610\n",
      "Epoch 00043 | Loss 0.4981 | Test Acc 0.3350 | Time(s) 0.1610\n",
      "Epoch 00044 | Loss 0.4789 | Test Acc 0.3350 | Time(s) 0.1610\n",
      "Epoch 00045 | Loss 0.4602 | Test Acc 0.3370 | Time(s) 0.1610\n",
      "Epoch 00046 | Loss 0.4421 | Test Acc 0.3410 | Time(s) 0.1610\n",
      "Epoch 00047 | Loss 0.4245 | Test Acc 0.3420 | Time(s) 0.1610\n",
      "Epoch 00048 | Loss 0.4074 | Test Acc 0.3430 | Time(s) 0.1610\n",
      "Epoch 00049 | Loss 0.3908 | Test Acc 0.3430 | Time(s) 0.1610\n",
      "Epoch 00050 | Loss 0.3748 | Test Acc 0.3430 | Time(s) 0.1610\n",
      "Epoch 00051 | Loss 0.3592 | Test Acc 0.3420 | Time(s) 0.1610\n",
      "Epoch 00052 | Loss 0.3442 | Test Acc 0.3410 | Time(s) 0.1610\n",
      "Epoch 00053 | Loss 0.3298 | Test Acc 0.3410 | Time(s) 0.1610\n",
      "Epoch 00054 | Loss 0.3156 | Test Acc 0.3400 | Time(s) 0.1610\n",
      "Epoch 00055 | Loss 0.3021 | Test Acc 0.3390 | Time(s) 0.1610\n",
      "Epoch 00056 | Loss 0.2890 | Test Acc 0.3390 | Time(s) 0.1610\n",
      "Epoch 00057 | Loss 0.2764 | Test Acc 0.3380 | Time(s) 0.1610\n",
      "Epoch 00058 | Loss 0.2644 | Test Acc 0.3350 | Time(s) 0.1610\n",
      "Epoch 00059 | Loss 0.2526 | Test Acc 0.3320 | Time(s) 0.1610\n",
      "Epoch 00060 | Loss 0.2414 | Test Acc 0.3290 | Time(s) 0.1610\n",
      "Epoch 00061 | Loss 0.2307 | Test Acc 0.3270 | Time(s) 0.1610\n",
      "Epoch 00062 | Loss 0.2205 | Test Acc 0.3230 | Time(s) 0.1610\n",
      "Epoch 00063 | Loss 0.2106 | Test Acc 0.3140 | Time(s) 0.1610\n",
      "Epoch 00064 | Loss 0.2013 | Test Acc 0.3000 | Time(s) 0.1610\n",
      "Epoch 00065 | Loss 0.1923 | Test Acc 0.2850 | Time(s) 0.1610\n",
      "Epoch 00066 | Loss 0.1838 | Test Acc 0.2790 | Time(s) 0.1610\n",
      "Epoch 00067 | Loss 0.1756 | Test Acc 0.2710 | Time(s) 0.1610\n",
      "Epoch 00068 | Loss 0.1679 | Test Acc 0.2650 | Time(s) 0.1610\n",
      "Epoch 00069 | Loss 0.1605 | Test Acc 0.2590 | Time(s) 0.1610\n",
      "Epoch 00070 | Loss 0.1535 | Test Acc 0.2510 | Time(s) 0.1610\n",
      "Epoch 00071 | Loss 0.1469 | Test Acc 0.2450 | Time(s) 0.1610\n",
      "Epoch 00072 | Loss 0.1406 | Test Acc 0.2410 | Time(s) 0.1610\n",
      "Epoch 00073 | Loss 0.1346 | Test Acc 0.2320 | Time(s) 0.1610\n",
      "Epoch 00074 | Loss 0.1289 | Test Acc 0.2280 | Time(s) 0.1610\n",
      "Epoch 00075 | Loss 0.1236 | Test Acc 0.2220 | Time(s) 0.1610\n",
      "Epoch 00076 | Loss 0.1184 | Test Acc 0.2150 | Time(s) 0.1610\n",
      "Epoch 00077 | Loss 0.1136 | Test Acc 0.2110 | Time(s) 0.1610\n",
      "Epoch 00078 | Loss 0.1091 | Test Acc 0.2040 | Time(s) 0.1610\n",
      "Epoch 00079 | Loss 0.1048 | Test Acc 0.2010 | Time(s) 0.1610\n",
      "Epoch 00080 | Loss 0.1007 | Test Acc 0.2000 | Time(s) 0.1610\n",
      "Epoch 00081 | Loss 0.0968 | Test Acc 0.2000 | Time(s) 0.1610\n",
      "Epoch 00082 | Loss 0.0931 | Test Acc 0.1960 | Time(s) 0.1610\n",
      "Epoch 00083 | Loss 0.0896 | Test Acc 0.1920 | Time(s) 0.1610\n",
      "Epoch 00084 | Loss 0.0863 | Test Acc 0.1910 | Time(s) 0.1610\n",
      "Epoch 00085 | Loss 0.0832 | Test Acc 0.1890 | Time(s) 0.1610\n",
      "Epoch 00086 | Loss 0.0802 | Test Acc 0.1860 | Time(s) 0.1610\n",
      "Epoch 00087 | Loss 0.0774 | Test Acc 0.1840 | Time(s) 0.1610\n",
      "Epoch 00088 | Loss 0.0748 | Test Acc 0.1820 | Time(s) 0.1610\n",
      "Epoch 00089 | Loss 0.0723 | Test Acc 0.1800 | Time(s) 0.1610\n",
      "Epoch 00090 | Loss 0.0699 | Test Acc 0.1770 | Time(s) 0.1610\n",
      "Epoch 00091 | Loss 0.0676 | Test Acc 0.1760 | Time(s) 0.1610\n",
      "Epoch 00092 | Loss 0.0654 | Test Acc 0.1760 | Time(s) 0.1610\n",
      "Epoch 00093 | Loss 0.0634 | Test Acc 0.1740 | Time(s) 0.1610\n",
      "Epoch 00094 | Loss 0.0614 | Test Acc 0.1740 | Time(s) 0.1610\n",
      "Epoch 00095 | Loss 0.0595 | Test Acc 0.1710 | Time(s) 0.1610\n",
      "Epoch 00096 | Loss 0.0577 | Test Acc 0.1690 | Time(s) 0.1610\n",
      "Epoch 00097 | Loss 0.0560 | Test Acc 0.1680 | Time(s) 0.1610\n",
      "Epoch 00098 | Loss 0.0544 | Test Acc 0.1670 | Time(s) 0.1610\n",
      "Epoch 00099 | Loss 0.0529 | Test Acc 0.1670 | Time(s) 0.1610\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "net = Net()\n",
    "g, features, labels, train_mask, test_mask = load_cora_data()\n",
    "train_mask = th.tensor([True] * 2 + [False] * (len(train_mask) - 2))\n",
    "optimizer = th.optim.Adam(net.parameters(), lr=1e-3)\n",
    "dur = []\n",
    "for epoch in range(100):\n",
    "    if epoch >=3:\n",
    "        t0 = time.time()\n",
    "\n",
    "    net.train()\n",
    "    logits = net(g, features)\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    loss = F.nll_loss(logp[train_mask], labels[train_mask])\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch >=3:\n",
    "        dur.append(time.time() - t0)\n",
    "\n",
    "    acc = evaluate(net, g, features, labels, test_mask)\n",
    "    print(\"Epoch {:05d} | Loss {:.4f} | Test Acc {:.4f} | Time(s) {:.4f}\".format(\n",
    "            epoch, loss.item(), acc, np.mean(dur)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = dgl.DGLGraph()\n",
    "S.from_networkx(G)\n",
    "S.ndata['feat'] = inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_nodes = np.random.choice(list(range(G.number_of_nodes())), 350)\n",
    "labels_train = labels[labeled_nodes]\n",
    "unlabelled_nodes = [i for i in list(range(G.number_of_nodes())) if i not in labeled_nodes]\n",
    "test_label = labels[unlabelled_nodes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | Loss: 2.3168\n",
      "Epoch 1 | Loss: 2.1854\n",
      "Epoch 2 | Loss: 2.0515\n",
      "Epoch 3 | Loss: 1.9082\n",
      "Epoch 4 | Loss: 1.7647\n",
      "Epoch 5 | Loss: 1.6287\n",
      "Epoch 6 | Loss: 1.4966\n",
      "Epoch 7 | Loss: 1.3618\n",
      "Epoch 8 | Loss: 1.2255\n",
      "Epoch 9 | Loss: 1.0934\n",
      "Epoch 10 | Loss: 0.9700\n",
      "Epoch 11 | Loss: 0.8562\n",
      "Epoch 12 | Loss: 0.7535\n",
      "Epoch 13 | Loss: 0.6632\n",
      "Epoch 14 | Loss: 0.5845\n",
      "Epoch 15 | Loss: 0.5139\n",
      "Epoch 16 | Loss: 0.4495\n",
      "Epoch 17 | Loss: 0.3933\n",
      "Epoch 18 | Loss: 0.3482\n",
      "Epoch 19 | Loss: 0.3133\n",
      "Epoch 20 | Loss: 0.2853\n",
      "Epoch 21 | Loss: 0.2632\n",
      "Epoch 22 | Loss: 0.2456\n",
      "Epoch 23 | Loss: 0.2296\n",
      "Epoch 24 | Loss: 0.2139\n",
      "Epoch 25 | Loss: 0.1989\n",
      "Epoch 26 | Loss: 0.1850\n",
      "Epoch 27 | Loss: 0.1718\n",
      "Epoch 28 | Loss: 0.1596\n",
      "Epoch 29 | Loss: 0.1497\n",
      "Epoch 30 | Loss: 0.1424\n",
      "Epoch 31 | Loss: 0.1371\n",
      "Epoch 32 | Loss: 0.1324\n",
      "Epoch 33 | Loss: 0.1274\n",
      "Epoch 34 | Loss: 0.1218\n",
      "Epoch 35 | Loss: 0.1162\n",
      "Epoch 36 | Loss: 0.1114\n",
      "Epoch 37 | Loss: 0.1080\n",
      "Epoch 38 | Loss: 0.1057\n",
      "Epoch 39 | Loss: 0.1037\n",
      "Epoch 40 | Loss: 0.1015\n",
      "Epoch 41 | Loss: 0.0988\n",
      "Epoch 42 | Loss: 0.0958\n",
      "Epoch 43 | Loss: 0.0931\n",
      "Epoch 44 | Loss: 0.0911\n",
      "Epoch 45 | Loss: 0.0897\n",
      "Epoch 46 | Loss: 0.0887\n",
      "Epoch 47 | Loss: 0.0875\n",
      "Epoch 48 | Loss: 0.0859\n",
      "Epoch 49 | Loss: 0.0842\n",
      "Epoch 50 | Loss: 0.0826\n",
      "Epoch 51 | Loss: 0.0814\n",
      "Epoch 52 | Loss: 0.0807\n",
      "Epoch 53 | Loss: 0.0800\n",
      "Epoch 54 | Loss: 0.0793\n",
      "Epoch 55 | Loss: 0.0784\n",
      "Epoch 56 | Loss: 0.0774\n",
      "Epoch 57 | Loss: 0.0765\n",
      "Epoch 58 | Loss: 0.0759\n",
      "Epoch 59 | Loss: 0.0755\n",
      "Epoch 60 | Loss: 0.0751\n",
      "Epoch 61 | Loss: 0.0746\n",
      "Epoch 62 | Loss: 0.0740\n",
      "Epoch 63 | Loss: 0.0734\n",
      "Epoch 64 | Loss: 0.0730\n",
      "Epoch 65 | Loss: 0.0726\n",
      "Epoch 66 | Loss: 0.0723\n",
      "Epoch 67 | Loss: 0.0720\n",
      "Epoch 68 | Loss: 0.0716\n",
      "Epoch 69 | Loss: 0.0712\n",
      "Epoch 70 | Loss: 0.0709\n",
      "Epoch 71 | Loss: 0.0706\n",
      "Epoch 72 | Loss: 0.0704\n",
      "Epoch 73 | Loss: 0.0701\n",
      "Epoch 74 | Loss: 0.0699\n",
      "Epoch 75 | Loss: 0.0696\n",
      "Epoch 76 | Loss: 0.0693\n",
      "Epoch 77 | Loss: 0.0691\n",
      "Epoch 78 | Loss: 0.0689\n",
      "Epoch 79 | Loss: 0.0687\n"
     ]
    }
   ],
   "source": [
    "net = GCN(G.number_of_nodes(), 64, np.unique(labels).shape[0])\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)\n",
    "all_logits = []\n",
    "for epoch in range(80):\n",
    "    logits = net(S, inputs)\n",
    "    # we save the logits for visualization later\n",
    "    all_logits.append(logits.detach())\n",
    "    logp = F.log_softmax(logits, 1)\n",
    "    # we only compute loss for labeled nodes\n",
    "    loss = F.nll_loss(logp[labeled_nodes], labels_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    print('Epoch %d | Loss: %.4f' % (epoch, loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.eval()\n",
    "logits = net(S, inputs)\n",
    "# we save the logits for visualization later\n",
    "logp = F.log_softmax(logits, 1)\n",
    "# we only compute loss for labeled nodes\n",
    "#loss = F.nll_loss(logp[labeled_nodes], labels_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "argmax_Y = torch.max(logp[unlabelled_nodes], 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of argmax predictions on the test set: 44.751381%\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy of argmax predictions on the test set: {:4f}%'.format(\n",
    "    (test_label == argmax_Y.float()).sum().item() / len(test_label) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
