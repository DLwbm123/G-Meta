{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_size=32, data_dir='./data/multiple_graph/cycle/META_LABEL/random_edge100/', epoch=10, fold_n=1, hidden_dim=32, input_dim=1, k_qry=12, k_spt=1, meta_lr=0.001, n_graphlets=5, n_way=2, no_finetune=True, task_num=4, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1067.25it/s]\n",
      "There are 1 non-isomorphic graphs\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1751.28it/s]\n",
      "There are 2 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 528.40it/s]\n",
      "There are 6 non-isomorphic graphs\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 37.12it/s]\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n",
      "Meta(\n",
      "  (net): Classifier(\n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x32]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (5): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (6): Parameter containing: [torch.FloatTensor of size 32x30]\n",
      "        (7): Parameter containing: [torch.FloatTensor of size 2x64]\n",
      "        (8): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (9): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (10): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (11): Parameter containing: [torch.FloatTensor of size 2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 4354\n",
      "shuffle DB :train, b:1000, 2-way, 1-shot, 12-query\n",
      "shuffle DB :val, b:100, 2-way, 1-shot, 12-query\n",
      "shuffle DB :test, b:100, 2-way, 1-shot, 12-query\n",
      "epoch: 0 step: 0 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 0 Val acc: [0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 0 step: 30 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 0 step: 60 \ttraining acc: [0.58333333 0.38541667 0.38541667 0.375      0.375      0.38541667]\n",
      "epoch: 0 step: 90 \ttraining acc: [0.5        0.39583333 0.26041667 0.20833333 0.4375     0.45833333]\n",
      "epoch: 0 step: 120 \ttraining acc: [0.51041667 0.4375     0.42708333 0.44791667 0.4375     0.42708333]\n",
      "epoch: 0 step: 150 \ttraining acc: [0.5        0.58333333 0.65625    0.63541667 0.57291667 0.55208333]\n",
      "epoch: 0 step: 180 \ttraining acc: [0.5        0.5        0.52083333 0.59375    0.55208333 0.54166667]\n",
      "epoch: 0 step: 210 \ttraining acc: [0.51041667 0.625      0.65625    0.67708333 0.70833333 0.67708333]\n",
      "epoch: 0 step: 240 \ttraining acc: [0.4375     0.60416667 0.5625     0.58333333 0.57291667 0.58333333]\n",
      "epoch: 1 step: 0 \ttraining acc: [0.48958333 0.61458333 0.61458333 0.59375    0.57291667 0.57291667]\n",
      "epoch: 1 Val acc: [0.5    0.5005 0.501  0.5044 0.501  0.4963 0.4937 0.4988 0.5015 0.5044\n",
      " 0.51  ]\n",
      "epoch: 1 step: 30 \ttraining acc: [0.5        0.54166667 0.53125    0.53125    0.55208333 0.54166667]\n",
      "epoch: 1 step: 60 \ttraining acc: [0.5     0.5     0.5     0.46875 0.46875 0.5    ]\n",
      "epoch: 1 step: 90 \ttraining acc: [0.4375     0.5625     0.51041667 0.48958333 0.51041667 0.48958333]\n",
      "epoch: 1 step: 120 \ttraining acc: [0.5        0.51041667 0.59375    0.61458333 0.64583333 0.61458333]\n",
      "epoch: 1 step: 150 \ttraining acc: [0.45833333 0.39583333 0.5        0.5        0.5        0.5       ]\n",
      "epoch: 1 step: 180 \ttraining acc: [0.63541667 0.64583333 0.63541667 0.67708333 0.67708333 0.70833333]\n",
      "epoch: 1 step: 210 \ttraining acc: [0.4375     0.4375     0.42708333 0.40625    0.44791667 0.44791667]\n",
      "epoch: 1 step: 240 \ttraining acc: [0.39583333 0.59375    0.65625    0.66666667 0.65625    0.67708333]\n",
      "epoch: 2 step: 0 \ttraining acc: [0.5        0.59375    0.60416667 0.59375    0.65625    0.64583333]\n",
      "epoch: 2 Val acc: [0.5    0.5    0.5024 0.4966 0.495  0.5005 0.506  0.509  0.5103 0.516\n",
      " 0.5156]\n",
      "epoch: 2 step: 30 \ttraining acc: [0.5        0.59375    0.65625    0.61458333 0.625      0.61458333]\n",
      "epoch: 2 step: 60 \ttraining acc: [0.55208333 0.6875     0.63541667 0.65625    0.65625    0.65625   ]\n",
      "epoch: 2 step: 90 \ttraining acc: [0.69791667 0.86458333 0.97916667 0.95833333 0.92708333 0.92708333]\n",
      "epoch: 2 step: 120 \ttraining acc: [0.5        0.58333333 0.5625     0.5625     0.61458333 0.59375   ]\n",
      "epoch: 2 step: 150 \ttraining acc: [0.5        0.58333333 0.54166667 0.52083333 0.5        0.54166667]\n",
      "epoch: 2 step: 180 \ttraining acc: [0.55208333 0.55208333 0.55208333 0.60416667 0.61458333 0.63541667]\n",
      "epoch: 2 step: 210 \ttraining acc: [0.5        0.59375    0.64583333 0.59375    0.5625     0.54166667]\n",
      "epoch: 2 step: 240 \ttraining acc: [0.48958333 0.48958333 0.45833333 0.47916667 0.59375    0.625     ]\n",
      "epoch: 3 step: 0 \ttraining acc: [0.66666667 0.69791667 0.70833333 0.76041667 0.78125    0.78125   ]\n",
      "epoch: 3 Val acc: [0.4988 0.49   0.4924 0.4922 0.4934 0.4954 0.4995 0.4934 0.4995 0.4912\n",
      " 0.497 ]\n",
      "epoch: 3 step: 30 \ttraining acc: [0.51041667 0.73958333 0.78125    0.83333333 0.82291667 0.82291667]\n",
      "epoch: 3 step: 60 \ttraining acc: [0.51041667 0.79166667 0.6875     0.59375    0.58333333 0.59375   ]\n",
      "epoch: 3 step: 90 \ttraining acc: [0.69791667 0.72916667 0.6875     0.71875    0.70833333 0.69791667]\n",
      "epoch: 3 step: 120 \ttraining acc: [0.55208333 0.57291667 0.5625     0.5625     0.55208333 0.53125   ]\n",
      "epoch: 3 step: 150 \ttraining acc: [0.375      0.47916667 0.625      0.70833333 0.80208333 0.8125    ]\n",
      "epoch: 3 step: 180 \ttraining acc: [0.57291667 0.55208333 0.55208333 0.5625     0.57291667 0.57291667]\n",
      "epoch: 3 step: 210 \ttraining acc: [0.42708333 0.69791667 0.8125     0.78125    0.8125     0.83333333]\n",
      "epoch: 3 step: 240 \ttraining acc: [0.5        0.52083333 0.69791667 0.70833333 0.70833333 0.69791667]\n",
      "epoch: 4 step: 0 \ttraining acc: [0.52083333 0.52083333 0.58333333 0.5625     0.54166667 0.51041667]\n",
      "epoch: 4 Val acc: [0.5    0.504  0.4922 0.495  0.5073 0.4983 0.506  0.5015 0.5073 0.503\n",
      " 0.5093]\n",
      "epoch: 4 step: 30 \ttraining acc: [0.54166667 0.70833333 0.73958333 0.73958333 0.73958333 0.73958333]\n",
      "epoch: 4 step: 60 \ttraining acc: [0.26041667 0.52083333 0.625      0.6875     0.70833333 0.72916667]\n",
      "epoch: 4 step: 90 \ttraining acc: [0.5        0.57291667 0.60416667 0.61458333 0.57291667 0.60416667]\n",
      "epoch: 4 step: 120 \ttraining acc: [0.5        0.66666667 0.75       0.73958333 0.72916667 0.71875   ]\n",
      "epoch: 4 step: 150 \ttraining acc: [0.53125    0.47916667 0.5625     0.47916667 0.52083333 0.52083333]\n",
      "epoch: 4 step: 180 \ttraining acc: [0.63541667 0.67708333 0.67708333 0.73958333 0.72916667 0.71875   ]\n",
      "epoch: 4 step: 210 \ttraining acc: [0.64583333 0.79166667 0.79166667 0.79166667 0.79166667 0.8125    ]\n",
      "epoch: 4 step: 240 \ttraining acc: [0.46875    0.61458333 0.60416667 0.60416667 0.625      0.65625   ]\n",
      "epoch: 5 step: 0 \ttraining acc: [0.4375     0.625      0.64583333 0.63541667 0.59375    0.60416667]\n",
      "epoch: 5 Val acc: [0.5    0.5034 0.4893 0.4883 0.4893 0.4866 0.4934 0.4988 0.5044 0.5044\n",
      " 0.507 ]\n",
      "epoch: 5 step: 30 \ttraining acc: [0.39583333 0.66666667 0.65625    0.6875     0.69791667 0.69791667]\n",
      "epoch: 5 step: 60 \ttraining acc: [0.5        0.71875    0.85416667 0.84375    0.79166667 0.83333333]\n",
      "epoch: 5 step: 90 \ttraining acc: [0.57291667 0.64583333 0.69791667 0.71875    0.78125    0.77083333]\n",
      "epoch: 5 step: 120 \ttraining acc: [0.54166667 0.64583333 0.6875     0.67708333 0.69791667 0.70833333]\n",
      "epoch: 5 step: 150 \ttraining acc: [0.48958333 0.5        0.54166667 0.52083333 0.42708333 0.53125   ]\n",
      "epoch: 5 step: 180 \ttraining acc: [0.5        0.55208333 0.53125    0.55208333 0.55208333 0.55208333]\n",
      "epoch: 5 step: 210 \ttraining acc: [0.5        0.42708333 0.48958333 0.41666667 0.48958333 0.46875   ]\n",
      "epoch: 5 step: 240 \ttraining acc: [0.5        0.60416667 0.51041667 0.5        0.52083333 0.55208333]\n",
      "epoch: 6 step: 0 \ttraining acc: [0.5     0.5     0.5     0.5     0.5     0.53125]\n",
      "epoch: 6 Val acc: [0.5    0.4866 0.5156 0.51   0.5117 0.5093 0.5073 0.513  0.512  0.5103\n",
      " 0.5103]\n",
      "epoch: 6 step: 30 \ttraining acc: [0.34375    0.42708333 0.42708333 0.4375     0.4375     0.44791667]\n",
      "epoch: 6 step: 60 \ttraining acc: [0.61458333 0.53125    0.53125    0.5625     0.59375    0.65625   ]\n",
      "epoch: 6 step: 90 \ttraining acc: [0.5        0.53125    0.60416667 0.57291667 0.67708333 0.69791667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 120 \ttraining acc: [0.5        0.5        0.48958333 0.57291667 0.625      0.69791667]\n",
      "epoch: 6 step: 150 \ttraining acc: [0.5        0.5        0.51041667 0.53125    0.58333333 0.58333333]\n",
      "epoch: 6 step: 180 \ttraining acc: [0.5        0.45833333 0.47916667 0.5        0.5        0.5       ]\n",
      "epoch: 6 step: 210 \ttraining acc: [0.44791667 0.5        0.5        0.5        0.5        0.46875   ]\n",
      "epoch: 6 step: 240 \ttraining acc: [0.5        0.4375     0.40625    0.40625    0.41666667 0.52083333]\n",
      "epoch: 7 step: 0 \ttraining acc: [0.5        0.375      0.46875    0.54166667 0.5        0.55208333]\n",
      "epoch: 7 Val acc: [0.5    0.4976 0.5034 0.501  0.5005 0.4963 0.4983 0.4946 0.493  0.4988\n",
      " 0.5   ]\n",
      "epoch: 7 step: 30 \ttraining acc: [0.5        0.58333333 0.54166667 0.59375    0.66666667 0.72916667]\n",
      "epoch: 7 step: 60 \ttraining acc: [0.52083333 0.65625    0.75       0.66666667 0.72916667 0.70833333]\n",
      "epoch: 7 step: 90 \ttraining acc: [0.53125 0.5     0.5     0.5     0.5625  0.59375]\n",
      "epoch: 7 step: 120 \ttraining acc: [0.5        0.5        0.5        0.52083333 0.55208333 0.55208333]\n",
      "epoch: 7 step: 150 \ttraining acc: [0.5        0.5        0.5        0.51041667 0.5625     0.5625    ]\n",
      "epoch: 7 step: 180 \ttraining acc: [0.5        0.5        0.5        0.5625     0.54166667 0.60416667]\n",
      "epoch: 7 step: 210 \ttraining acc: [0.5625     0.51041667 0.53125    0.5        0.69791667 0.61458333]\n",
      "epoch: 7 step: 240 \ttraining acc: [0.5        0.69791667 0.72916667 0.71875    0.71875    0.71875   ]\n",
      "epoch: 8 step: 0 \ttraining acc: [0.5        0.51041667 0.69791667 0.70833333 0.70833333 0.70833333]\n",
      "epoch: 8 Val acc: [0.5    0.5    0.4988 0.4993 0.4983 0.4988 0.5    0.497  0.501  0.502\n",
      " 0.5073]\n",
      "epoch: 8 step: 30 \ttraining acc: [0.5        0.51041667 0.53125    0.52083333 0.5        0.51041667]\n",
      "epoch: 8 step: 60 \ttraining acc: [0.5        0.5        0.51041667 0.5        0.5        0.58333333]\n",
      "epoch: 8 step: 90 \ttraining acc: [0.5        0.61458333 0.54166667 0.4375     0.53125    0.58333333]\n",
      "epoch: 8 step: 120 \ttraining acc: [0.5        0.5        0.45833333 0.48958333 0.48958333 0.48958333]\n",
      "epoch: 8 step: 150 \ttraining acc: [0.5625     0.46875    0.55208333 0.61458333 0.64583333 0.6875    ]\n",
      "epoch: 8 step: 180 \ttraining acc: [0.5        0.5        0.44791667 0.58333333 0.57291667 0.58333333]\n",
      "epoch: 8 step: 210 \ttraining acc: [0.5        0.5        0.52083333 0.44791667 0.41666667 0.42708333]\n",
      "epoch: 8 step: 240 \ttraining acc: [0.625      0.67708333 0.76041667 0.77083333 0.8125     0.8125    ]\n",
      "epoch: 9 step: 0 \ttraining acc: [0.55208333 0.52083333 0.59375    0.61458333 0.67708333 0.71875   ]\n",
      "epoch: 9 Val acc: [0.5    0.4905 0.5005 0.5093 0.5107 0.5107 0.5044 0.5    0.4995 0.4978\n",
      " 0.5024]\n",
      "epoch: 9 step: 30 \ttraining acc: [0.41666667 0.53125    0.625      0.64583333 0.73958333 0.83333333]\n",
      "epoch: 9 step: 60 \ttraining acc: [0.67708333 0.70833333 0.80208333 0.83333333 0.8125     0.8125    ]\n",
      "epoch: 9 step: 90 \ttraining acc: [0.4375     0.35416667 0.47916667 0.55208333 0.55208333 0.55208333]\n",
      "epoch: 9 step: 120 \ttraining acc: [0.42708333 0.52083333 0.48958333 0.47916667 0.47916667 0.48958333]\n",
      "epoch: 9 step: 150 \ttraining acc: [0.45833333 0.45833333 0.70833333 0.71875    0.76041667 0.80208333]\n",
      "epoch: 9 step: 180 \ttraining acc: [0.55208333 0.48958333 0.51041667 0.5        0.53125    0.52083333]\n",
      "epoch: 9 step: 210 \ttraining acc: [0.5        0.66666667 0.80208333 0.85416667 0.875      0.875     ]\n",
      "epoch: 9 step: 240 \ttraining acc: [0.45833333 0.41666667 0.55208333 0.48958333 0.53125    0.67708333]\n",
      "Test acc: [0.5    0.5156 0.6206 0.7026 0.6714 0.7065 0.679  0.7056 0.685  0.712\n",
      " 0.703 ]\n"
     ]
    }
   ],
   "source": [
    "!python ./graphlet_meta_label/train.py \\\n",
    "        --data_dir ./data/multiple_graph/cycle/META_LABEL/random_edge100/ \\\n",
    "        --fold_n 1 \\\n",
    "        --k_spt 1 \\\n",
    "        --k_qry 12 \\\n",
    "        --epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_size=32, data_dir='./data/multiple_graph/cycle/META_LABEL/random_edge100/', epoch=10, fold_n=3, hidden_dim=32, input_dim=1, k_qry=12, k_spt=1, meta_lr=0.001, n_graphlets=5, n_way=2, no_finetune=True, task_num=4, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "100%|███████████████████████████████████████████| 1/1 [00:00<00:00, 1002.94it/s]\n",
      "There are 1 non-isomorphic graphs\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1680.75it/s]\n",
      "There are 2 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 471.75it/s]\n",
      "There are 6 non-isomorphic graphs\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 35.57it/s]\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n",
      "Meta(\n",
      "  (net): Classifier(\n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x32]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (5): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (6): Parameter containing: [torch.FloatTensor of size 32x30]\n",
      "        (7): Parameter containing: [torch.FloatTensor of size 2x64]\n",
      "        (8): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (9): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (10): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (11): Parameter containing: [torch.FloatTensor of size 2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 4354\n",
      "shuffle DB :train, b:1000, 2-way, 1-shot, 12-query\n",
      "shuffle DB :val, b:100, 2-way, 1-shot, 12-query\n",
      "shuffle DB :test, b:100, 2-way, 1-shot, 12-query\n",
      "epoch: 0 step: 0 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 0 Val acc: [0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5    0.5\n",
      " 0.4983]\n",
      "epoch: 0 step: 30 \ttraining acc: [0.5        0.5        0.54166667 0.625      0.61458333 0.60416667]\n",
      "epoch: 0 step: 60 \ttraining acc: [0.54166667 0.51041667 0.51041667 0.52083333 0.54166667 0.52083333]\n",
      "epoch: 0 step: 90 \ttraining acc: [0.375   0.4375  0.53125 0.53125 0.5     0.5    ]\n",
      "epoch: 0 step: 120 \ttraining acc: [0.5        0.61458333 0.59375    0.53125    0.5        0.5       ]\n",
      "epoch: 0 step: 150 \ttraining acc: [0.59375    0.52083333 0.47916667 0.4375     0.45833333 0.47916667]\n",
      "epoch: 0 step: 180 \ttraining acc: [0.44791667 0.4375     0.42708333 0.44791667 0.5625     0.57291667]\n",
      "epoch: 0 step: 210 \ttraining acc: [0.53125    0.60416667 0.625      0.625      0.625      0.61458333]\n",
      "epoch: 0 step: 240 \ttraining acc: [0.69791667 0.69791667 0.66666667 0.72916667 0.72916667 0.71875   ]\n",
      "epoch: 1 step: 0 \ttraining acc: [0.55208333 0.48958333 0.46875    0.57291667 0.63541667 0.65625   ]\n",
      "epoch: 1 Val acc: [0.4958 0.4995 0.4963 0.5015 0.5093 0.508  0.508  0.5117 0.518  0.5156\n",
      " 0.5176]\n",
      "epoch: 1 step: 30 \ttraining acc: [0.53125    0.54166667 0.55208333 0.52083333 0.59375    0.625     ]\n",
      "epoch: 1 step: 60 \ttraining acc: [0.60416667 0.70833333 0.6875     0.63541667 0.625      0.60416667]\n",
      "epoch: 1 step: 90 \ttraining acc: [0.78125    0.78125    0.51041667 0.5        0.5        0.5       ]\n",
      "epoch: 1 step: 120 \ttraining acc: [0.5        0.5        0.47916667 0.53125    0.4375     0.45833333]\n",
      "epoch: 1 step: 150 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 1 step: 180 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 1 step: 210 \ttraining acc: [0.36458333 0.55208333 0.5        0.5        0.5        0.5       ]\n",
      "epoch: 1 step: 240 \ttraining acc: [0.48958333 0.45833333 0.5        0.5        0.51041667 0.51041667]\n",
      "epoch: 2 step: 0 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 2 Val acc: [0.5    0.5    0.5    0.5015 0.5015 0.4995 0.5034 0.501  0.5024 0.501\n",
      " 0.4983]\n",
      "epoch: 2 step: 30 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 2 step: 60 \ttraining acc: [0.5        0.76041667 0.67708333 0.55208333 0.5        0.5       ]\n",
      "epoch: 2 step: 90 \ttraining acc: [0.5        0.5        0.54166667 0.5        0.5        0.5       ]\n",
      "epoch: 2 step: 120 \ttraining acc: [0.36458333 0.59375    0.5625     0.55208333 0.54166667 0.54166667]\n",
      "epoch: 2 step: 150 \ttraining acc: [0.5        0.44791667 0.41666667 0.45833333 0.5        0.5       ]\n",
      "epoch: 2 step: 180 \ttraining acc: [0.5        0.46875    0.5        0.48958333 0.48958333 0.5       ]\n",
      "epoch: 2 step: 210 \ttraining acc: [0.44791667 0.57291667 0.64583333 0.64583333 0.65625    0.67708333]\n",
      "epoch: 2 step: 240 \ttraining acc: [0.5        0.53125    0.67708333 0.63541667 0.625      0.60416667]\n",
      "epoch: 3 step: 0 \ttraining acc: [0.5        0.64583333 0.82291667 0.82291667 0.83333333 0.75      ]\n",
      "epoch: 3 Val acc: [0.4734 0.4893 0.4941 0.5054 0.5063 0.5137 0.5107 0.5107 0.503  0.5117\n",
      " 0.508 ]\n",
      "epoch: 3 step: 30 \ttraining acc: [0.82291667 0.86458333 0.86458333 0.85416667 0.85416667 0.85416667]\n",
      "epoch: 3 step: 60 \ttraining acc: [0.30208333 0.39583333 0.44791667 0.46875    0.47916667 0.47916667]\n",
      "epoch: 3 step: 90 \ttraining acc: [0.25       0.35416667 0.4375     0.42708333 0.45833333 0.44791667]\n",
      "epoch: 3 step: 120 \ttraining acc: [0.1875     0.20833333 0.27083333 0.32291667 0.375      0.44791667]\n",
      "epoch: 3 step: 150 \ttraining acc: [0.53125    0.45833333 0.53125    0.55208333 0.52083333 0.53125   ]\n",
      "epoch: 3 step: 180 \ttraining acc: [0.34375    0.36458333 0.47916667 0.58333333 0.66666667 0.63541667]\n",
      "epoch: 3 step: 210 \ttraining acc: [0.4375     0.38541667 0.5        0.5        0.5        0.5       ]\n",
      "epoch: 3 step: 240 \ttraining acc: [0.52083333 0.64583333 0.61458333 0.57291667 0.5625     0.5625    ]\n",
      "epoch: 4 step: 0 \ttraining acc: [0.625      0.57291667 0.55208333 0.58333333 0.57291667 0.5625    ]\n",
      "epoch: 4 Val acc: [0.4805 0.4836 0.4746 0.4824 0.4875 0.4976 0.5005 0.4993 0.497  0.4937\n",
      " 0.4937]\n",
      "epoch: 4 step: 30 \ttraining acc: [0.5        0.66666667 0.625      0.55208333 0.5        0.5       ]\n",
      "epoch: 4 step: 60 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 4 step: 90 \ttraining acc: [0.58333333 0.58333333 0.5        0.5        0.5        0.5       ]\n",
      "epoch: 4 step: 120 \ttraining acc: [0.61458333 0.67708333 0.60416667 0.58333333 0.58333333 0.58333333]\n",
      "epoch: 4 step: 150 \ttraining acc: [0.20833333 0.38541667 0.4375     0.44791667 0.44791667 0.45833333]\n",
      "epoch: 4 step: 180 \ttraining acc: [0.42708333 0.54166667 0.51041667 0.5        0.51041667 0.5       ]\n",
      "epoch: 4 step: 210 \ttraining acc: [0.30208333 0.55208333 0.5625     0.5625     0.5625     0.5625    ]\n",
      "epoch: 4 step: 240 \ttraining acc: [0.44791667 0.61458333 0.65625    0.69791667 0.70833333 0.6875    ]\n",
      "epoch: 5 step: 0 \ttraining acc: [0.59375    0.57291667 0.61458333 0.60416667 0.58333333 0.57291667]\n",
      "epoch: 5 Val acc: [0.4995 0.4988 0.5073 0.5073 0.5073 0.5103 0.5146 0.512  0.5083 0.5073\n",
      " 0.5073]\n",
      "epoch: 5 step: 30 \ttraining acc: [0.48958333 0.54166667 0.58333333 0.58333333 0.59375    0.60416667]\n",
      "epoch: 5 step: 60 \ttraining acc: [0.51041667 0.41666667 0.34375    0.40625    0.41666667 0.40625   ]\n",
      "epoch: 5 step: 90 \ttraining acc: [0.46875    0.39583333 0.5        0.46875    0.46875    0.46875   ]\n",
      "epoch: 5 step: 120 \ttraining acc: [0.29166667 0.28125    0.38541667 0.45833333 0.47916667 0.5       ]\n",
      "epoch: 5 step: 150 \ttraining acc: [0.5        0.70833333 0.83333333 0.80208333 0.78125    0.77083333]\n",
      "epoch: 5 step: 180 \ttraining acc: [0.4375     0.42708333 0.5        0.53125    0.52083333 0.57291667]\n",
      "epoch: 5 step: 210 \ttraining acc: [0.4375     0.78125    0.82291667 0.77083333 0.76041667 0.75      ]\n",
      "epoch: 5 step: 240 \ttraining acc: [0.52083333 0.76041667 0.70833333 0.66666667 0.64583333 0.64583333]\n",
      "epoch: 6 step: 0 \ttraining acc: [0.46875    0.59375    0.51041667 0.47916667 0.42708333 0.42708333]\n",
      "epoch: 6 Val acc: [0.5    0.503  0.5093 0.5083 0.5186 0.5254 0.525  0.5244 0.53   0.5293\n",
      " 0.5254]\n",
      "epoch: 6 step: 30 \ttraining acc: [0.40625    0.53125    0.63541667 0.63541667 0.63541667 0.63541667]\n",
      "epoch: 6 step: 60 \ttraining acc: [0.625      0.57291667 0.54166667 0.58333333 0.61458333 0.60416667]\n",
      "epoch: 6 step: 90 \ttraining acc: [0.5        0.67708333 0.64583333 0.58333333 0.58333333 0.58333333]\n",
      "epoch: 6 step: 120 \ttraining acc: [0.21875    0.35416667 0.45833333 0.5        0.5        0.5       ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 150 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 6 step: 180 \ttraining acc: [0.52083333 0.5        0.5        0.5        0.5        0.5       ]\n",
      "epoch: 6 step: 210 \ttraining acc: [0.5        0.51041667 0.5        0.5        0.5        0.5       ]\n",
      "epoch: 6 step: 240 \ttraining acc: [0.3125     0.41666667 0.42708333 0.42708333 0.41666667 0.41666667]\n",
      "epoch: 7 step: 0 \ttraining acc: [0.71875    0.54166667 0.58333333 0.5625     0.5625     0.5625    ]\n",
      "epoch: 7 Val acc: [0.5015 0.5103 0.5093 0.5073 0.509  0.507  0.5073 0.5093 0.5127 0.5156\n",
      " 0.514 ]\n",
      "epoch: 7 step: 30 \ttraining acc: [0.5        0.48958333 0.53125    0.52083333 0.48958333 0.48958333]\n",
      "epoch: 7 step: 60 \ttraining acc: [0.55208333 0.53125    0.54166667 0.5625     0.59375    0.5625    ]\n",
      "epoch: 7 step: 90 \ttraining acc: [0.5        0.47916667 0.58333333 0.55208333 0.5625     0.55208333]\n",
      "epoch: 7 step: 120 \ttraining acc: [0.34375    0.39583333 0.4375     0.4375     0.4375     0.4375    ]\n",
      "epoch: 7 step: 150 \ttraining acc: [0.36458333 0.40625    0.61458333 0.61458333 0.61458333 0.625     ]\n",
      "epoch: 7 step: 180 \ttraining acc: [0.5625     0.54166667 0.67708333 0.66666667 0.64583333 0.64583333]\n",
      "epoch: 7 step: 210 \ttraining acc: [0.5        0.5        0.53125    0.58333333 0.57291667 0.59375   ]\n",
      "epoch: 7 step: 240 \ttraining acc: [0.52083333 0.58333333 0.58333333 0.57291667 0.53125    0.52083333]\n",
      "epoch: 8 step: 0 \ttraining acc: [0.55208333 0.4375     0.58333333 0.61458333 0.61458333 0.60416667]\n",
      "epoch: 8 Val acc: [0.4937 0.483  0.5137 0.5234 0.521  0.5186 0.5117 0.509  0.5054 0.5015\n",
      " 0.5005]\n",
      "epoch: 8 step: 30 \ttraining acc: [0.5        0.5        0.5        0.51041667 0.59375    0.66666667]\n",
      "epoch: 8 step: 60 \ttraining acc: [0.60416667 0.60416667 0.77083333 0.80208333 0.73958333 0.77083333]\n",
      "epoch: 8 step: 90 \ttraining acc: [0.38541667 0.30208333 0.5        0.51041667 0.54166667 0.53125   ]\n",
      "epoch: 8 step: 120 \ttraining acc: [0.59375    0.61458333 0.67708333 0.6875     0.65625    0.63541667]\n",
      "epoch: 8 step: 150 \ttraining acc: [0.36458333 0.45833333 0.51041667 0.51041667 0.54166667 0.54166667]\n",
      "epoch: 8 step: 180 \ttraining acc: [0.5        0.55208333 0.5        0.5        0.5        0.52083333]\n",
      "epoch: 8 step: 210 \ttraining acc: [0.40625    0.41666667 0.4375     0.5        0.5        0.5       ]\n",
      "epoch: 8 step: 240 \ttraining acc: [0.35416667 0.53125    0.52083333 0.52083333 0.52083333 0.52083333]\n",
      "epoch: 9 step: 0 \ttraining acc: [0.75       0.73958333 0.72916667 0.72916667 0.69791667 0.6875    ]\n",
      "epoch: 9 Val acc: [0.5117 0.502  0.5166 0.5356 0.538  0.5327 0.532  0.5293 0.5273 0.527\n",
      " 0.5225]\n",
      "epoch: 9 step: 30 \ttraining acc: [0.40625    0.54166667 0.57291667 0.5625     0.57291667 0.57291667]\n",
      "epoch: 9 step: 60 \ttraining acc: [0.61458333 0.58333333 0.60416667 0.59375    0.59375    0.59375   ]\n",
      "epoch: 9 step: 90 \ttraining acc: [0.5        0.48958333 0.5        0.52083333 0.54166667 0.54166667]\n",
      "epoch: 9 step: 120 \ttraining acc: [0.61458333 0.65625    0.64583333 0.65625    0.66666667 0.6875    ]\n",
      "epoch: 9 step: 150 \ttraining acc: [0.54166667 0.65625    0.64583333 0.66666667 0.70833333 0.71875   ]\n",
      "epoch: 9 step: 180 \ttraining acc: [0.75       0.6875     0.72916667 0.75       0.77083333 0.77083333]\n",
      "epoch: 9 step: 210 \ttraining acc: [0.46875    0.72916667 0.66666667 0.6875     0.69791667 0.67708333]\n",
      "epoch: 9 step: 240 \ttraining acc: [0.47916667 0.625      0.60416667 0.60416667 0.60416667 0.61458333]\n",
      "Test acc: [0.5    0.517  0.4812 0.5664 0.6016 0.6387 0.655  0.6694 0.6777 0.684\n",
      " 0.6934]\n"
     ]
    }
   ],
   "source": [
    "!python ./graphlet_meta_label/train.py \\\n",
    "        --data_dir ./data/multiple_graph/cycle/META_LABEL/random_edge100/ \\\n",
    "        --fold_n 3 \\\n",
    "        --k_spt 1 \\\n",
    "        --k_qry 12 \\\n",
    "        --epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_size=32, data_dir='./data/multiple_graph/cycle/META_LABEL/random_edge100/', epoch=10, fold_n=1, hidden_dim=32, input_dim=1, k_qry=12, k_spt=1, meta_lr=0.01, n_graphlets=5, n_way=2, no_finetune=True, task_num=1, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 264.47it/s]\n",
      "There are 1 non-isomorphic graphs\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1536.09it/s]\n",
      "There are 2 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 495.17it/s]\n",
      "There are 6 non-isomorphic graphs\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 36.99it/s]\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n",
      "Meta(\n",
      "  (net): Classifier(\n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x32]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (5): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (6): Parameter containing: [torch.FloatTensor of size 32x30]\n",
      "        (7): Parameter containing: [torch.FloatTensor of size 2x64]\n",
      "        (8): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (9): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (10): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (11): Parameter containing: [torch.FloatTensor of size 2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 4354\n",
      "shuffle DB :train, b:1000, 2-way, 1-shot, 12-query\n",
      "shuffle DB :val, b:100, 2-way, 1-shot, 12-query\n",
      "shuffle DB :test, b:100, 2-way, 1-shot, 12-query\n",
      "epoch: 0 step: 0 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 0 Val acc: [0.501  0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044\n",
      " 0.5044]\n",
      "epoch: 0 step: 30 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 0 step: 90 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 0 step: 120 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 0 step: 150 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 0 step: 180 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 210 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 0 step: 240 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 270 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 0 step: 300 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 0 step: 330 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 0 step: 360 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 0 step: 390 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 0 step: 420 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 0 step: 450 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 0 step: 480 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 0 Val acc: [0.501  0.501  0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015\n",
      " 0.5015]\n",
      "epoch: 0 step: 510 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 0 step: 540 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 570 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 600 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 0 step: 630 \ttraining acc: [0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334]\n",
      "epoch: 0 step: 660 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 0 step: 690 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 0 step: 720 \ttraining acc: [0.7916667 0.7916667 0.75      0.7916667 0.75      0.7916667]\n",
      "epoch: 0 step: 750 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 0 step: 780 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 0 step: 810 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 0 step: 840 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 0 step: 870 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 0 step: 900 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 0 step: 930 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 0 step: 960 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 990 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 0 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 Val acc: [0.5    0.5005 0.5005 0.5005 0.5005 0.501  0.5015 0.5015 0.5015 0.5015\n",
      " 0.5015]\n",
      "epoch: 1 step: 30 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 60 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 90 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 120 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 1 step: 150 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 1 step: 180 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 1 step: 210 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 240 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 270 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 1 step: 300 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 330 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 1 step: 360 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 1 step: 390 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 1 step: 420 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 450 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 Val acc: [0.501  0.501  0.501  0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015\n",
      " 0.5015]\n",
      "epoch: 1 step: 510 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 540 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 1 step: 570 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 600 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 630 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 660 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 1 step: 690 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 720 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 750 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 1 step: 780 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 1 step: 810 \ttraining acc: [0.29166666 0.29166666 0.29166666 0.29166666 0.29166666 0.29166666]\n",
      "epoch: 1 step: 840 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 1 step: 870 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 900 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 930 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 1 step: 960 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 990 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 0 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 2 Val acc: [0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015\n",
      " 0.5015]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 step: 30 \ttraining acc: [0.29166666 0.29166666 0.29166666 0.29166666 0.29166666 0.29166666]\n",
      "epoch: 2 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 90 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 2 step: 120 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 150 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 2 step: 180 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 2 step: 210 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 240 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 2 step: 270 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 300 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 2 step: 330 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 360 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 2 step: 390 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 2 step: 420 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 2 step: 450 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 Val acc: [0.5    0.5005 0.5005 0.5005 0.5005 0.5    0.5    0.5    0.5    0.501\n",
      " 0.501 ]\n",
      "epoch: 2 step: 510 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 540 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 2 step: 570 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 2 step: 600 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 2 step: 630 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 2 step: 660 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 2 step: 690 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 720 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 750 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 780 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 2 step: 810 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 2 step: 840 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 870 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 900 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 2 step: 930 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 2 step: 960 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 2 step: 990 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 0 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 Val acc: [0.501  0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.5015 0.501\n",
      " 0.501 ]\n",
      "epoch: 3 step: 30 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 60 \ttraining acc: [0.375 0.375 0.375 0.375 0.375 0.375]\n",
      "epoch: 3 step: 90 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 3 step: 120 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 step: 150 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 180 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 3 step: 210 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 3 step: 240 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 3 step: 270 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 3 step: 300 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 330 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 3 step: 360 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 390 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 420 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 3 step: 450 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 step: 480 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 3 Val acc: [0.5005 0.501  0.501  0.5    0.5    0.5005 0.5005 0.5015 0.5015 0.5015\n",
      " 0.5005]\n",
      "epoch: 3 step: 510 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 540 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 570 \ttraining acc: [0.29166666 0.29166666 0.29166666 0.29166666 0.29166666 0.29166666]\n",
      "epoch: 3 step: 600 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 3 step: 630 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 660 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 690 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 720 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 3 step: 750 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 3 step: 780 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 3 step: 810 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 3 step: 840 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 3 step: 870 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 3 step: 900 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 3 step: 930 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 960 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 990 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 4 step: 0 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 4 Val acc: [0.501  0.501  0.501  0.501  0.501  0.501  0.501  0.501  0.501  0.5005\n",
      " 0.5005]\n",
      "epoch: 4 step: 30 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 4 step: 60 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 4 step: 90 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 4 step: 120 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 150 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 180 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 210 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 4 step: 240 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 4 step: 270 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 4 step: 300 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 4 step: 330 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 360 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 4 step: 390 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 4 step: 420 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 450 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 480 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 Val acc: [0.5034 0.5034 0.5034 0.5034 0.5034 0.5034 0.504  0.504  0.504  0.504\n",
      " 0.504 ]\n",
      "epoch: 4 step: 510 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 4 step: 540 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 4 step: 570 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 600 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 4 step: 630 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 660 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 690 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 4 step: 720 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 4 step: 750 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 4 step: 780 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 810 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 840 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 870 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 4 step: 900 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 4 step: 930 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 960 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 990 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 0 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 5 Val acc: [0.5044 0.5034 0.504  0.504  0.504  0.504  0.504  0.504  0.504  0.504\n",
      " 0.504 ]\n",
      "epoch: 5 step: 30 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 5 step: 60 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 90 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 5 step: 120 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 150 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 5 step: 180 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 5 step: 210 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 240 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 5 step: 270 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 300 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 330 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 360 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 390 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 420 \ttraining acc: [0.375 0.375 0.375 0.375 0.375 0.375]\n",
      "epoch: 5 step: 450 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 5 step: 480 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 5 Val acc: [0.5044 0.504  0.504  0.504  0.504  0.5044 0.5044 0.5044 0.5044 0.5044\n",
      " 0.5044]\n",
      "epoch: 5 step: 510 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 5 step: 540 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 570 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 600 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 5 step: 630 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 5 step: 660 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 690 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 720 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 5 step: 750 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 780 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 810 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 840 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 5 step: 870 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 900 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 5 step: 930 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 5 step: 960 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 990 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 0 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 6 Val acc: [0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044\n",
      " 0.5044]\n",
      "epoch: 6 step: 30 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 90 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 120 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 150 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 180 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 210 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 240 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 270 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 300 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 6 step: 330 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 360 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 390 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 420 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 450 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 480 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 Val acc: [0.5044 0.504  0.504  0.504  0.504  0.504  0.5044 0.5044 0.5044 0.5044\n",
      " 0.5044]\n",
      "epoch: 6 step: 510 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 540 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 570 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 600 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 6 step: 630 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 6 step: 660 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 690 \ttraining acc: [0.08333334 0.08333334 0.08333334 0.08333334 0.08333334 0.08333334]\n",
      "epoch: 6 step: 720 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 750 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 6 step: 780 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 810 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 840 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 870 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 900 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 6 step: 930 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 6 step: 960 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 990 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 7 step: 0 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 7 Val acc: [0.506 0.506 0.506 0.506 0.506 0.506 0.506 0.506 0.506 0.506 0.506]\n",
      "epoch: 7 step: 30 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 7 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 90 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 120 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 7 step: 150 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 180 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 210 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 240 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 7 step: 270 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 300 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 7 step: 330 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 7 step: 360 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 390 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 420 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 450 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 Val acc: [0.5044 0.504  0.504  0.504  0.504  0.504  0.504  0.5044 0.5044 0.5044\n",
      " 0.5044]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 510 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 7 step: 540 \ttraining acc: [0.375 0.375 0.375 0.375 0.375 0.375]\n",
      "epoch: 7 step: 570 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 7 step: 600 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 630 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 7 step: 660 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 7 step: 690 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 720 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 750 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 780 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 7 step: 810 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 7 step: 840 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 870 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 900 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 7 step: 930 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 7 step: 960 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 7 step: 990 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 8 step: 0 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 Val acc: [0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044\n",
      " 0.5044]\n",
      "epoch: 8 step: 30 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 8 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 90 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 120 \ttraining acc: [0.29166666 0.29166666 0.29166666 0.29166666 0.29166666 0.29166666]\n",
      "epoch: 8 step: 150 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 180 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 8 step: 210 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 240 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 8 step: 270 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 8 step: 300 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 8 step: 330 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 360 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 390 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 8 step: 420 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 450 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 480 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 8 Val acc: [0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054\n",
      " 0.5054]\n",
      "epoch: 8 step: 510 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 8 step: 540 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 8 step: 570 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 8 step: 600 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 8 step: 630 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 8 step: 660 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 8 step: 690 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 8 step: 720 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 750 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 780 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 810 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 8 step: 840 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 8 step: 870 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 8 step: 900 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 8 step: 930 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 8 step: 960 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 8 step: 990 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 9 step: 0 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 9 Val acc: [0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054 0.5054\n",
      " 0.5054]\n",
      "epoch: 9 step: 30 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 9 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 90 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 120 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 9 step: 150 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 180 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 210 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 240 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 9 step: 270 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 300 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 9 step: 330 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 9 step: 360 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 9 step: 390 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 9 step: 420 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 450 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 9 step: 480 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 9 Val acc: [0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044 0.5044\n",
      " 0.5044]\n",
      "epoch: 9 step: 510 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 9 step: 540 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 570 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 9 step: 600 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 9 step: 630 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 660 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 9 step: 690 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 9 step: 720 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 9 step: 750 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 780 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 9 step: 810 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 840 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 870 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 9 step: 900 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 930 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 9 step: 960 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 990 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "Test acc: [0.8623 0.8623 0.8623 0.8623 0.8623 0.8623 0.8623 0.8623 0.8623 0.8623\n",
      " 0.8623]\n"
     ]
    }
   ],
   "source": [
    "!python ./graphlet_Proto2_label/train.py \\\n",
    "        --data_dir ./data/multiple_graph/cycle/META_LABEL/random_edge100/ \\\n",
    "        --fold_n 1 \\\n",
    "        --k_spt 1 \\\n",
    "        --k_qry 12 \\\n",
    "        --meta_lr 1e-2 \\\n",
    "        --update_lr 0.01 \\\n",
    "        --epoch 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_size=32, data_dir='./data/multiple_graph/cycle/META_LABEL/random_edge100/', epoch=10, fold_n=3, hidden_dim=32, input_dim=1, k_qry=12, k_spt=3, meta_lr=0.01, n_graphlets=5, n_way=2, no_finetune=True, task_num=1, update_lr=0.01, update_step=5, update_step_test=10)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 901.81it/s]\n",
      "There are 1 non-isomorphic graphs\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1771.24it/s]\n",
      "There are 2 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 556.16it/s]\n",
      "There are 6 non-isomorphic graphs\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 37.26it/s]\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n",
      "Meta(\n",
      "  (net): Classifier(\n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x32]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (5): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (6): Parameter containing: [torch.FloatTensor of size 32x30]\n",
      "        (7): Parameter containing: [torch.FloatTensor of size 2x64]\n",
      "        (8): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (9): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (10): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (11): Parameter containing: [torch.FloatTensor of size 2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 4354\n",
      "shuffle DB :train, b:1000, 2-way, 3-shot, 12-query\n",
      "shuffle DB :val, b:100, 2-way, 3-shot, 12-query\n",
      "shuffle DB :test, b:100, 2-way, 3-shot, 12-query\n",
      "epoch: 0 step: 0 \ttraining acc: [0.5       0.5       0.7083333 0.75      0.75      0.75     ]\n",
      "epoch: 0 Val acc: [0.5376 0.539  0.539  0.539  0.5386 0.5386 0.5386 0.5386 0.5386 0.5386\n",
      " 0.539 ]\n",
      "epoch: 0 step: 30 \ttraining acc: [0.75      0.75      0.8333333 1.        0.9583333 0.9583333]\n",
      "epoch: 0 step: 60 \ttraining acc: [0.7916667 0.7916667 0.5833333 0.5       0.5       0.6666667]\n",
      "epoch: 0 step: 90 \ttraining acc: [0.7083333 0.7083333 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 0 step: 120 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 150 \ttraining acc: [0.5       0.5       0.5       0.5       0.5       0.5833333]\n",
      "epoch: 0 step: 180 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.875     0.875     0.875    ]\n",
      "epoch: 0 step: 210 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 0 step: 240 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 270 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.75      0.75     ]\n",
      "epoch: 0 step: 300 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 330 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 360 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 0 step: 390 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 420 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 450 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 0 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 0 Val acc: [0.5356 0.536  0.5376 0.536  0.5356 0.539  0.537  0.5366 0.5356 0.5376\n",
      " 0.5376]\n",
      "epoch: 0 step: 510 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 0 step: 540 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 0 step: 570 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 0 step: 600 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 0 step: 630 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 0 step: 660 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 0 step: 690 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 0 step: 720 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 750 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 0 step: 780 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 0 step: 810 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 840 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 0 step: 870 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 0 step: 900 \ttraining acc: [1.        1.        1.        1.        1.        0.9583333]\n",
      "epoch: 0 step: 930 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 0 step: 960 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 0 step: 990 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 1 step: 0 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.5        0.5       ]\n",
      "epoch: 1 Val acc: [0.5283 0.5317 0.5327 0.532  0.532  0.5312 0.531  0.5303 0.531  0.531\n",
      " 0.5312]\n",
      "epoch: 1 step: 30 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 60 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 90 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 1.        1.       ]\n",
      "epoch: 1 step: 120 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 1 step: 150 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 1 step: 180 \ttraining acc: [0.75      0.75      0.75      0.75      0.75      0.7916667]\n",
      "epoch: 1 step: 210 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 1 step: 240 \ttraining acc: [0.75      0.75      0.7083333 0.5833333 0.7916667 0.7916667]\n",
      "epoch: 1 step: 270 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 300 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 1 step: 330 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 1 step: 360 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 390 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 1 step: 420 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 450 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 1 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 1.        1.       ]\n",
      "epoch: 1 Val acc: [0.5303 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327\n",
      " 0.533 ]\n",
      "epoch: 1 step: 510 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 1 step: 540 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 570 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 1 step: 600 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 630 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 1 step: 660 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 1 step: 690 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 1 step: 720 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 1 step: 750 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 780 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 810 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 1 step: 840 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 1 step: 870 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 900 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 1 step: 930 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 1 step: 960 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 1 step: 990 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 2 step: 0 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 2 Val acc: [0.538  0.539  0.539  0.539  0.539  0.539  0.539  0.5386 0.539  0.539\n",
      " 0.54  ]\n",
      "epoch: 2 step: 30 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 60 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 2 step: 90 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.45833334 0.5       ]\n",
      "epoch: 2 step: 120 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 150 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 2 step: 180 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 210 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9583333]\n",
      "epoch: 2 step: 240 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 2 step: 270 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 300 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.5        0.5        0.5       ]\n",
      "epoch: 2 step: 330 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 360 \ttraining acc: [0.5       0.5       0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 2 step: 390 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 2 step: 420 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.875    ]\n",
      "epoch: 2 step: 450 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 Val acc: [0.5293 0.527  0.5283 0.5283 0.529  0.529  0.5293 0.53   0.5293 0.5293\n",
      " 0.529 ]\n",
      "epoch: 2 step: 510 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 540 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 570 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 2 step: 600 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 2 step: 630 \ttraining acc: [0.29166666 0.29166666 0.29166666 0.29166666 0.29166666 0.29166666]\n",
      "epoch: 2 step: 660 \ttraining acc: [0.5       0.5       0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 2 step: 690 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 2 step: 720 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 750 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 2 step: 780 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 2 step: 810 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.5        0.625     ]\n",
      "epoch: 2 step: 840 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 2 step: 870 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 2 step: 900 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 2 step: 930 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 2 step: 960 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 2 step: 990 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 3 step: 0 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 3 Val acc: [0.5293 0.5283 0.529  0.529  0.5283 0.528  0.5283 0.5283 0.53   0.53\n",
      " 0.53  ]\n",
      "epoch: 3 step: 30 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 60 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 3 step: 90 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 step: 120 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5833333 0.5416667]\n",
      "epoch: 3 step: 150 \ttraining acc: [0.625     0.625     0.6666667 0.75      0.75      0.7916667]\n",
      "epoch: 3 step: 180 \ttraining acc: [0.75      0.75      0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 3 step: 210 \ttraining acc: [0.5       0.5       0.5       0.5416667 0.5       0.5      ]\n",
      "epoch: 3 step: 240 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 270 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 3 step: 300 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 3 step: 330 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 360 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 390 \ttraining acc: [0.9583333 0.9583333 1.        1.        1.        1.       ]\n",
      "epoch: 3 step: 420 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 3 step: 450 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 3 step: 480 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 Val acc: [0.5293 0.53   0.5303 0.5303 0.53   0.53   0.53   0.53   0.53   0.5293\n",
      " 0.5293]\n",
      "epoch: 3 step: 510 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 3 step: 540 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 3 step: 570 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 3 step: 600 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 3 step: 630 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 660 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 step: 690 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 3 step: 720 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 step: 750 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 3 step: 780 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 810 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 3 step: 840 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 3 step: 870 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 900 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 930 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 3 step: 960 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 3 step: 990 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 4 step: 0 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.5833333 ]\n",
      "epoch: 4 Val acc: [0.553  0.5537 0.554  0.5537 0.553  0.5522 0.551  0.551  0.551  0.5503\n",
      " 0.5503]\n",
      "epoch: 4 step: 30 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 4 step: 60 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 90 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 4 step: 120 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 150 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 4 step: 180 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 4 step: 210 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 240 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 270 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 300 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 330 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 360 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 4 step: 390 \ttraining acc: [0.8333333 0.8333333 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 420 \ttraining acc: [0.5416667 0.5416667 0.5833333 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 4 step: 450 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 4 step: 480 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 4 Val acc: [0.54   0.5405 0.5405 0.541  0.541  0.5405 0.541  0.541  0.5405 0.54\n",
      " 0.5405]\n",
      "epoch: 4 step: 510 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 540 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.41666666]\n",
      "epoch: 4 step: 570 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 4 step: 600 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 630 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 660 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 690 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 720 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 4 step: 750 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 780 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 4 step: 810 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 4 step: 840 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 4 step: 870 \ttraining acc: [0.7083333 0.7083333 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 4 step: 900 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 4 step: 930 \ttraining acc: [0.7083333 0.7083333 0.75      0.6666667 0.9166667 0.9166667]\n",
      "epoch: 4 step: 960 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 4 step: 990 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 0 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 5 Val acc: [0.53   0.5327 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327 0.5327\n",
      " 0.5327]\n",
      "epoch: 5 step: 30 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 60 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 90 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 5 step: 120 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 150 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 5 step: 180 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 5 step: 210 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 240 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 270 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 5 step: 300 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 5 step: 330 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 5 step: 360 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 5 step: 390 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 420 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 450 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 5 step: 480 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 5 Val acc: [0.535  0.537  0.537  0.538  0.5376 0.5376 0.537  0.5376 0.537  0.537\n",
      " 0.5376]\n",
      "epoch: 5 step: 510 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 540 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 5 step: 570 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 5 step: 600 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 630 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 5 step: 660 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 5 step: 690 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 5 step: 720 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 750 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 5 step: 780 \ttraining acc: [0.7083333 0.7083333 0.75      0.75      0.75      0.75     ]\n",
      "epoch: 5 step: 810 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9583333]\n",
      "epoch: 5 step: 840 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 5 step: 870 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.375      0.375     ]\n",
      "epoch: 5 step: 900 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 5 step: 930 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 5 step: 960 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 5 step: 990 \ttraining acc: [0.20833333 0.20833333 0.20833333 0.20833333 0.20833333 0.20833333]\n",
      "epoch: 6 step: 0 \ttraining acc: [0.875     0.875     0.875     0.875     0.875     0.9166667]\n",
      "epoch: 6 Val acc: [0.5327 0.5347 0.534  0.534  0.534  0.534  0.5337 0.534  0.534  0.5347\n",
      " 0.534 ]\n",
      "epoch: 6 step: 30 \ttraining acc: [0.7083333 0.7083333 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 6 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 90 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 120 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 150 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 6 step: 180 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 6 step: 210 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 6 step: 240 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 6 step: 270 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 300 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 6 step: 330 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 360 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 390 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 6 step: 420 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 450 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 480 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 6 Val acc: [0.53   0.531  0.5303 0.5303 0.531  0.531  0.53   0.53   0.53   0.53\n",
      " 0.53  ]\n",
      "epoch: 6 step: 510 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 6 step: 540 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 570 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 6 step: 600 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 630 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 660 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 6 step: 690 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 720 \ttraining acc: [1.        1.        1.        1.        1.        0.9583333]\n",
      "epoch: 6 step: 750 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 780 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 6 step: 810 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 840 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 6 step: 870 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 6 step: 900 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 930 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 6 step: 960 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 6 step: 990 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 7 step: 0 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 Val acc: [0.5303 0.5356 0.5347 0.5347 0.5347 0.5347 0.5347 0.535  0.535  0.5347\n",
      " 0.534 ]\n",
      "epoch: 7 step: 30 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 7 step: 60 \ttraining acc: [0.5416667  0.5416667  0.5        0.41666666 0.5        0.5       ]\n",
      "epoch: 7 step: 90 \ttraining acc: [0.5       0.5       0.5416667 0.625     0.5       0.5833333]\n",
      "epoch: 7 step: 120 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 7 step: 150 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 180 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 210 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 240 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 7 step: 270 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 300 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 7 step: 330 \ttraining acc: [0.7083333 0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 7 step: 360 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 390 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 7 step: 420 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 7 step: 450 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 7 step: 480 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 7 Val acc: [0.5366 0.538  0.541  0.543  0.5444 0.55   0.553  0.548  0.547  0.548\n",
      " 0.5503]\n",
      "epoch: 7 step: 510 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 7 step: 540 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 570 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 7 step: 600 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 7 step: 630 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 7 step: 660 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 7 step: 690 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 7 step: 720 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 7 step: 750 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 780 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 810 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 7 step: 840 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 7 step: 870 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 900 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.75      0.6666667 0.75     ]\n",
      "epoch: 7 step: 930 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 7 step: 960 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 7 step: 990 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 0 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 Val acc: [0.516  0.5215 0.523  0.5264 0.5327 0.536  0.54   0.542  0.542  0.544\n",
      " 0.54  ]\n",
      "epoch: 8 step: 30 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 8 step: 60 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 90 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 8 step: 120 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 8 step: 150 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 8 step: 180 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 8 step: 210 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 8 step: 240 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 8 step: 270 \ttraining acc: [0.75      0.75      0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 8 step: 300 \ttraining acc: [0.375 0.375 0.375 0.375 0.375 0.375]\n",
      "epoch: 8 step: 330 \ttraining acc: [1.        1.        1.        1.        1.        0.9583333]\n",
      "epoch: 8 step: 360 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 390 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 420 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 8 step: 450 \ttraining acc: [0.5833333 0.5833333 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 8 step: 480 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 8 Val acc: [0.5356 0.533  0.5337 0.534  0.5337 0.534  0.535  0.5327 0.5312 0.535\n",
      " 0.535 ]\n",
      "epoch: 8 step: 510 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 8 step: 540 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 570 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 600 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 8 step: 630 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 8 step: 660 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 8 step: 690 \ttraining acc: [0.625     0.625     0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 8 step: 720 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 750 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 8 step: 780 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 810 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 840 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 870 \ttraining acc: [0.625 0.625 0.625 0.625 0.625 0.625]\n",
      "epoch: 8 step: 900 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 8 step: 930 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 8 step: 960 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 8 step: 990 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 9 step: 0 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 9 Val acc: [0.5293 0.527  0.5273 0.5283 0.5283 0.5283 0.5293 0.53   0.53   0.53\n",
      " 0.5293]\n",
      "epoch: 9 step: 30 \ttraining acc: [0.875 0.875 0.875 0.875 0.875 0.875]\n",
      "epoch: 9 step: 60 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 90 \ttraining acc: [0.45833334 0.45833334 0.45833334 0.45833334 0.45833334 0.45833334]\n",
      "epoch: 9 step: 120 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 9 step: 150 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 9 step: 180 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 210 \ttraining acc: [0.5416667 0.5416667 0.5833333 0.5833333 0.5833333 0.5833333]\n",
      "epoch: 9 step: 240 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 270 \ttraining acc: [0.5416667 0.5416667 0.5416667 0.5416667 0.5416667 0.5416667]\n",
      "epoch: 9 step: 300 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 9 step: 330 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 9 step: 360 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 390 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.875     0.875     0.875    ]\n",
      "epoch: 9 step: 420 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 450 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9583333 0.9583333]\n",
      "epoch: 9 step: 480 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 Val acc: [0.5283 0.53   0.53   0.5293 0.5293 0.5293 0.53   0.53   0.5303 0.5303\n",
      " 0.5303]\n",
      "epoch: 9 step: 510 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 540 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.875    ]\n",
      "epoch: 9 step: 570 \ttraining acc: [0.33333334 0.33333334 0.33333334 0.33333334 0.33333334 0.33333334]\n",
      "epoch: 9 step: 600 \ttraining acc: [0.9166667 0.9166667 0.9166667 0.9166667 0.9166667 0.9166667]\n",
      "epoch: 9 step: 630 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 660 \ttraining acc: [0.41666666 0.41666666 0.41666666 0.41666666 0.41666666 0.41666666]\n",
      "epoch: 9 step: 690 \ttraining acc: [0.9583333 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 step: 720 \ttraining acc: [1.        1.        1.        1.        1.        0.9583333]\n",
      "epoch: 9 step: 750 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 9 step: 780 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 9 step: 810 \ttraining acc: [0.6666667 0.6666667 0.6666667 0.6666667 0.6666667 0.6666667]\n",
      "epoch: 9 step: 840 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "epoch: 9 step: 870 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 9 step: 900 \ttraining acc: [0.5 0.5 0.5 0.5 0.5 0.5]\n",
      "epoch: 9 step: 930 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 9 step: 960 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 9 step: 990 \ttraining acc: [1. 1. 1. 1. 1. 1.]\n",
      "Test acc: [0.9844 0.984  0.984  0.984  0.984  0.984  0.984  0.984  0.984  0.984\n",
      " 0.984 ]\n"
     ]
    }
   ],
   "source": [
    "!python ./graphlet_Proto2_label/train.py \\\n",
    "        --data_dir ./data/multiple_graph/cycle/META_LABEL/random_edge100/ \\\n",
    "        --fold_n 3 \\\n",
    "        --k_spt 3 \\\n",
    "        --k_qry 12 \\\n",
    "        --meta_lr 1e-2 \\\n",
    "        --update_lr 0.01 \\\n",
    "        --epoch 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_size=32, data_dir='./data/multiple_graph/cycle/META_LABEL/random_edge100/', epoch=10, fold_n=1, hidden_dim=32, input_dim=1, k_qry=12, k_spt=1, meta_lr=0.005, n_graphlets=5, n_way=2, no_finetune=True, task_num=4, update_lr=0.05, update_step=5, update_step_test=30)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 819.52it/s]\n",
      "There are 1 non-isomorphic graphs\n",
      "100%|███████████████████████████████████████████| 2/2 [00:00<00:00, 1414.13it/s]\n",
      "There are 2 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 492.61it/s]\n",
      "There are 6 non-isomorphic graphs\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 38.62it/s]\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n",
      "Meta(\n",
      "  (net): Classifier(\n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x32]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (5): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (6): Parameter containing: [torch.FloatTensor of size 32x30]\n",
      "        (7): Parameter containing: [torch.FloatTensor of size 2x64]\n",
      "        (8): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (9): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (10): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (11): Parameter containing: [torch.FloatTensor of size 2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 4354\n",
      "shuffle DB :train, b:1000, 2-way, 1-shot, 12-query\n",
      "shuffle DB :val, b:100, 2-way, 1-shot, 12-query\n",
      "shuffle DB :test, b:100, 2-way, 1-shot, 12-query\n",
      "epoch: 0 step: 0 \ttraining acc: [0.6041667 0.6041667 0.625     0.6458334 0.6666667 0.6666667]\n",
      "epoch: 0 Val acc: [0.5015 0.5015 0.501  0.5024 0.502  0.505  0.5054 0.504  0.504  0.5054\n",
      " 0.5054 0.5054 0.507  0.5073 0.5083 0.5093 0.5083 0.508  0.5083 0.5073\n",
      " 0.5073 0.508  0.5073 0.508  0.507  0.5063 0.506  0.5063 0.5073 0.5063\n",
      " 0.507 ]\n",
      "epoch: 0 step: 30 \ttraining acc: [0.9270833 0.7083333 0.7395833 0.7604167 0.7708334 0.8125   ]\n",
      "epoch: 0 step: 60 \ttraining acc: [0.7395833  0.7708334  0.7708334  0.7708334  0.78125006 0.7916667 ]\n",
      "epoch: 0 step: 90 \ttraining acc: [0.7916666  0.84374994 0.8333333  0.8333333  0.8541666  0.84375   ]\n",
      "epoch: 0 step: 120 \ttraining acc: [0.6458333 0.6354166 0.6354166 0.6354166 0.6458333 0.6458333]\n",
      "epoch: 0 step: 150 \ttraining acc: [0.78125   0.7291666 0.7291666 0.7291666 0.7291666 0.7395833]\n",
      "epoch: 0 step: 180 \ttraining acc: [0.7291666  0.6979167  0.71875006 0.71875006 0.71875006 0.7291667 ]\n",
      "epoch: 0 step: 210 \ttraining acc: [0.8958334 0.90625   0.90625   0.8958334 0.8958334 0.8958334]\n",
      "epoch: 0 step: 240 \ttraining acc: [0.9791666 0.96875   0.96875   0.96875   0.96875   0.96875  ]\n",
      "epoch: 1 step: 0 \ttraining acc: [0.7395833 0.71875   0.71875   0.71875   0.71875   0.71875  ]\n",
      "epoch: 1 Val acc: [0.516  0.5156 0.516  0.518  0.5205 0.5205 0.5205 0.5215 0.5225 0.5215\n",
      " 0.522  0.5234 0.5234 0.524  0.5244 0.5234 0.5234 0.5244 0.524  0.524\n",
      " 0.525  0.5264 0.5283 0.528  0.5273 0.527  0.528  0.528  0.528  0.528\n",
      " 0.528 ]\n",
      "epoch: 1 step: 30 \ttraining acc: [0.8125    0.8020834 0.8020834 0.8020834 0.8020834 0.8020834]\n",
      "epoch: 1 step: 60 \ttraining acc: [0.6145834  0.6041667  0.6041667  0.6041667  0.6041667  0.59375006]\n",
      "epoch: 1 step: 90 \ttraining acc: [0.8125 0.8125 0.8125 0.8125 0.8125 0.8125]\n",
      "epoch: 1 step: 120 \ttraining acc: [0.7395833 0.7708333 0.7604167 0.7708333 0.7708333 0.7604167]\n",
      "epoch: 1 step: 150 \ttraining acc: [0.74999994 0.7916667  0.8125     0.8229167  0.8125     0.8125    ]\n",
      "epoch: 1 step: 180 \ttraining acc: [0.6979167 0.71875   0.6979167 0.6979167 0.6875    0.7083334]\n",
      "epoch: 1 step: 210 \ttraining acc: [0.6875  0.65625 0.65625 0.65625 0.65625 0.65625]\n",
      "epoch: 1 step: 240 \ttraining acc: [0.8854167  0.8333334  0.8645834  0.8333334  0.87500006 0.87500006]\n",
      "epoch: 2 step: 0 \ttraining acc: [0.7916666 0.8333333 0.8333333 0.8333333 0.8333333 0.84375  ]\n",
      "epoch: 2 Val acc: [0.5327 0.531  0.5317 0.5312 0.5312 0.531  0.529  0.53   0.5327 0.535\n",
      " 0.534  0.535  0.535  0.538  0.54   0.544  0.5425 0.538  0.538  0.5386\n",
      " 0.5386 0.5386 0.5356 0.536  0.5356 0.534  0.5347 0.5347 0.5356 0.5356\n",
      " 0.5347]\n",
      "epoch: 2 step: 30 \ttraining acc: [0.6770833 0.6041666 0.6041666 0.6041666 0.6145833 0.6458333]\n",
      "epoch: 2 step: 60 \ttraining acc: [0.7708333 0.7708333 0.7708333 0.7708333 0.7708333 0.7708333]\n",
      "epoch: 2 step: 90 \ttraining acc: [0.90624994 0.9479167  0.9479167  0.9479167  0.9479167  0.9479167 ]\n",
      "epoch: 2 step: 120 \ttraining acc: [0.6875    0.7291667 0.7395834 0.71875   0.71875   0.7083334]\n",
      "epoch: 2 step: 150 \ttraining acc: [0.7708333 0.78125   0.78125   0.7916667 0.7916667 0.7916667]\n",
      "epoch: 2 step: 180 \ttraining acc: [0.6979167 0.65625   0.65625   0.65625   0.6458334 0.6458334]\n",
      "epoch: 2 step: 210 \ttraining acc: [0.7916667 0.75      0.75      0.7604166 0.7604166 0.7708333]\n",
      "epoch: 2 step: 240 \ttraining acc: [0.7708334 0.7916667 0.7916667 0.78125   0.7708334 0.7708334]\n",
      "epoch: 3 step: 0 \ttraining acc: [0.65625   0.6979167 0.6979167 0.7291667 0.6979167 0.6770833]\n",
      "epoch: 3 Val acc: [0.5444 0.5522 0.5527 0.5527 0.5474 0.55   0.55   0.55   0.55   0.5513\n",
      " 0.551  0.5527 0.5513 0.551  0.5527 0.5527 0.552  0.5503 0.5522 0.5527\n",
      " 0.5527 0.551  0.551  0.551  0.5527 0.5527 0.5527 0.5527 0.5522 0.5527\n",
      " 0.5527]\n",
      "epoch: 3 step: 30 \ttraining acc: [0.8229167 0.7708334 0.8125    0.8125    0.8229167 0.8229167]\n",
      "epoch: 3 step: 60 \ttraining acc: [0.9166666 0.9166666 0.9166666 0.9166666 0.9166666 0.9166666]\n",
      "epoch: 3 step: 90 \ttraining acc: [0.6458334 0.6666667 0.65625   0.65625   0.65625   0.65625  ]\n",
      "epoch: 3 step: 120 \ttraining acc: [0.90625 0.90625 0.90625 0.90625 0.90625 0.90625]\n",
      "epoch: 3 step: 150 \ttraining acc: [0.8125    0.8020833 0.8020833 0.8125    0.8125    0.8125   ]\n",
      "epoch: 3 step: 180 \ttraining acc: [0.8020833 0.7083333 0.7604167 0.7708334 0.7708334 0.7708334]\n",
      "epoch: 3 step: 210 \ttraining acc: [0.78124994 0.8020833  0.7916666  0.8020833  0.81249994 0.8229166 ]\n",
      "epoch: 3 step: 240 \ttraining acc: [0.8958333 0.75      0.8333333 0.84375   0.84375   0.84375  ]\n",
      "epoch: 4 step: 0 \ttraining acc: [0.7708333 0.7291666 0.7708333 0.7708333 0.7708333 0.7708333]\n",
      "epoch: 4 Val acc: [0.5776 0.6025 0.5903 0.5786 0.584  0.583  0.5884 0.584  0.5923 0.589\n",
      " 0.589  0.586  0.591  0.5864 0.5894 0.589  0.592  0.591  0.588  0.5923\n",
      " 0.592  0.5903 0.5913 0.59   0.596  0.589  0.5913 0.592  0.5938 0.5894\n",
      " 0.5957]\n",
      "epoch: 4 step: 30 \ttraining acc: [0.8958333 0.875     0.8958333 0.8958333 0.8958333 0.8958333]\n",
      "epoch: 4 step: 60 \ttraining acc: [0.90625   0.9166667 0.9166667 0.9166667 0.9166667 0.90625  ]\n",
      "epoch: 4 step: 90 \ttraining acc: [0.6354167 0.59375   0.6145834 0.6145834 0.625     0.6145834]\n",
      "epoch: 4 step: 120 \ttraining acc: [0.8645833 0.7395833 0.8645834 0.8541667 0.8541667 0.8645833]\n",
      "epoch: 4 step: 150 \ttraining acc: [0.6770834 0.7708333 0.7604167 0.7395833 0.7291667 0.71875  ]\n",
      "epoch: 4 step: 180 \ttraining acc: [0.7604166 0.7708333 0.7708333 0.7708334 0.78125   0.78125  ]\n",
      "epoch: 4 step: 210 \ttraining acc: [0.9270833 0.9270833 0.9166667 0.9270833 0.9270833 0.8854167]\n",
      "epoch: 4 step: 240 \ttraining acc: [0.7604167  0.7708334  0.7708334  0.78125006 0.8229167  0.8229167 ]\n",
      "epoch: 5 step: 0 \ttraining acc: [0.75      0.7291667 0.7395833 0.7291667 0.7395833 0.7291667]\n",
      "epoch: 5 Val acc: [0.555  0.586  0.5723 0.569  0.578  0.57   0.571  0.5728 0.5796 0.5674\n",
      " 0.572  0.572  0.5674 0.5703 0.5806 0.5635 0.5728 0.5693 0.571  0.571\n",
      " 0.5728 0.5674 0.5723 0.57   0.5747 0.5713 0.572  0.5723 0.571  0.57\n",
      " 0.57  ]\n",
      "epoch: 5 step: 30 \ttraining acc: [0.78125006 0.7395833  0.7291667  0.71875    0.6875     0.75      ]\n",
      "epoch: 5 step: 60 \ttraining acc: [0.8958333  0.8125     0.8645833  0.8958333  0.8854166  0.93749994]\n",
      "epoch: 5 step: 90 \ttraining acc: [0.6875    0.6979167 0.6979167 0.6875    0.6979167 0.6875   ]\n",
      "epoch: 5 step: 120 \ttraining acc: [0.8020833 0.78125   0.8020833 0.7604167 0.75      0.7604167]\n",
      "epoch: 5 step: 150 \ttraining acc: [0.7708334 0.7916667 0.75      0.78125   0.7604167 0.7395834]\n",
      "epoch: 5 step: 180 \ttraining acc: [0.7604167  0.7708334  0.7708334  0.7604167  0.78125006 0.7708334 ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 5 step: 210 \ttraining acc: [0.75      0.7395833 0.75      0.7395833 0.7395833 0.7395833]\n",
      "epoch: 5 step: 240 \ttraining acc: [0.90625   0.90625   0.90625   0.90625   0.90625   0.9166667]\n",
      "epoch: 6 step: 0 \ttraining acc: [0.7916667  0.78125006 0.8125     0.8125     0.8125     0.8125    ]\n",
      "epoch: 6 Val acc: [0.594  0.6045 0.6064 0.6064 0.6094 0.6064 0.6143 0.605  0.6157 0.61\n",
      " 0.618  0.608  0.6147 0.6084 0.615  0.608  0.6104 0.609  0.612  0.615\n",
      " 0.6143 0.6113 0.6157 0.609  0.6157 0.6104 0.6123 0.611  0.608  0.6104\n",
      " 0.6133]\n",
      "epoch: 6 step: 30 \ttraining acc: [0.8333333 0.8333333 0.8229167 0.8229167 0.8125    0.8125   ]\n",
      "epoch: 6 step: 60 \ttraining acc: [0.71875   0.6666666 0.71875   0.6666666 0.71875   0.6666666]\n",
      "epoch: 6 step: 90 \ttraining acc: [0.7395834  0.75000006 0.7395834  0.7395834  0.7395834  0.7395834 ]\n",
      "epoch: 6 step: 120 \ttraining acc: [0.4791667  0.46875    0.4895833  0.47916666 0.47916666 0.47916666]\n",
      "epoch: 6 step: 150 \ttraining acc: [0.7916667  0.78125    0.78125006 0.7708334  0.7916667  0.7708334 ]\n",
      "epoch: 6 step: 180 \ttraining acc: [0.8125     0.7604166  0.7708333  0.78124994 0.78124994 0.78124994]\n",
      "epoch: 6 step: 210 \ttraining acc: [0.75      0.7083334 0.7083334 0.7083334 0.71875   0.71875  ]\n",
      "epoch: 6 step: 240 \ttraining acc: [0.8333333 0.7395833 0.75      0.75      0.75      0.75     ]\n",
      "epoch: 7 step: 0 \ttraining acc: [0.7916667 0.75      0.7604167 0.7604167 0.7604167 0.7604167]\n",
      "epoch: 7 Val acc: [0.5527 0.5776 0.579  0.5796 0.581  0.583  0.5825 0.5835 0.5835 0.5806\n",
      " 0.579  0.578  0.5767 0.577  0.577  0.5728 0.574  0.5737 0.573  0.5737\n",
      " 0.5723 0.5723 0.5713 0.5728 0.573  0.573  0.573  0.574  0.575  0.5767\n",
      " 0.578 ]\n",
      "epoch: 7 step: 30 \ttraining acc: [0.84375   0.78125   0.78125   0.78125   0.78125   0.7916667]\n",
      "epoch: 7 step: 60 \ttraining acc: [0.9375    0.9583333 0.9895833 0.9895833 0.9895833 0.9895833]\n",
      "epoch: 7 step: 90 \ttraining acc: [0.8645834 0.8020833 0.8020833 0.8125    0.8229167 0.8229167]\n",
      "epoch: 7 step: 120 \ttraining acc: [0.8854166  0.8645834  0.8645834  0.87500006 0.87500006 0.87500006]\n",
      "epoch: 7 step: 150 \ttraining acc: [0.8541666  0.84375006 0.84375    0.84375    0.8229167  0.84375   ]\n",
      "epoch: 7 step: 180 \ttraining acc: [0.7708333 0.7395833 0.7395833 0.7395833 0.7395833 0.7395833]\n",
      "epoch: 7 step: 210 \ttraining acc: [0.7395833  0.71875    0.75000006 0.75000006 0.75000006 0.75000006]\n",
      "epoch: 7 step: 240 \ttraining acc: [0.9270833 0.8854167 0.9166667 0.9166667 0.9166667 0.9270833]\n",
      "epoch: 8 step: 0 \ttraining acc: [0.8541667 0.9166667 0.9479167 0.9270833 0.9166667 0.90625  ]\n",
      "epoch: 8 Val acc: [0.569  0.595  0.5903 0.5854 0.582  0.582  0.581  0.581  0.581  0.5786\n",
      " 0.5776 0.5776 0.577  0.578  0.577  0.578  0.5776 0.576  0.577  0.576\n",
      " 0.5767 0.577  0.574  0.5728 0.575  0.577  0.576  0.5757 0.577  0.5776\n",
      " 0.5776]\n",
      "epoch: 8 step: 30 \ttraining acc: [0.5416667 0.5104167 0.5104167 0.5104167 0.5       0.5      ]\n",
      "epoch: 8 step: 60 \ttraining acc: [0.7604166 0.7291666 0.7291666 0.7395833 0.7395833 0.7395833]\n",
      "epoch: 8 step: 90 \ttraining acc: [0.8958333 0.8645834 0.8541667 0.8541667 0.8645833 0.8645833]\n",
      "epoch: 8 step: 120 \ttraining acc: [0.7916667  0.7604166  0.7916667  0.84374994 0.8333333  0.8541666 ]\n",
      "epoch: 8 step: 150 \ttraining acc: [0.9583333  0.93749994 0.90624994 0.8541666  0.8541666  0.90624994]\n",
      "epoch: 8 step: 180 \ttraining acc: [0.7916667 0.8020833 0.7916667 0.8229166 0.8333333 0.8333333]\n",
      "epoch: 8 step: 210 \ttraining acc: [0.90625   0.8333333 0.8958333 0.8854167 0.90625   0.90625  ]\n",
      "epoch: 8 step: 240 \ttraining acc: [0.9375    0.8854167 0.875     0.8854167 0.8854167 0.8854167]\n",
      "epoch: 9 step: 0 \ttraining acc: [0.9583333  0.9583333  0.93749994 0.9583333  0.9583333  0.9583333 ]\n",
      "epoch: 9 Val acc: [0.5894 0.575  0.584  0.597  0.591  0.6025 0.6074 0.6025 0.595  0.589\n",
      " 0.596  0.59   0.58   0.59   0.5776 0.604  0.6064 0.607  0.601  0.615\n",
      " 0.607  0.6167 0.6035 0.6304 0.616  0.6177 0.6133 0.6216 0.6323 0.6313\n",
      " 0.6367]\n",
      "epoch: 9 step: 30 \ttraining acc: [0.9583333 0.9375    0.9375    0.9375    0.9583333 0.9583333]\n",
      "epoch: 9 step: 60 \ttraining acc: [0.9166667 0.9166667 0.90625   0.9166667 0.9166667 0.9166667]\n",
      "epoch: 9 step: 90 \ttraining acc: [0.9166666 0.875     0.875     0.875     0.875     0.875    ]\n",
      "epoch: 9 step: 120 \ttraining acc: [0.875     0.8020833 0.8645833 0.8333333 0.84375   0.84375  ]\n",
      "epoch: 9 step: 150 \ttraining acc: [0.8854166 0.84375   0.84375   0.8541667 0.8645833 0.875    ]\n",
      "epoch: 9 step: 180 \ttraining acc: [0.7708333  0.7083333  0.75000006 0.7604167  0.7604167  0.7604167 ]\n",
      "epoch: 9 step: 210 \ttraining acc: [0.96875    0.9270833  0.8958333  0.84374994 0.9791667  0.9791667 ]\n",
      "epoch: 9 step: 240 \ttraining acc: [0.7395833  0.6770833  0.68749994 0.65625    0.6770833  0.6770833 ]\n",
      "Test acc: [0.78   0.7173 0.7383 0.774  0.8203 0.816  0.8374 0.85   0.845  0.862\n",
      " 0.8677 0.871  0.873  0.873  0.8813 0.885  0.8945 0.8867 0.894  0.8906\n",
      " 0.8945 0.892  0.896  0.8916 0.894  0.8906 0.8945 0.898  0.8965 0.898\n",
      " 0.898 ]\n"
     ]
    }
   ],
   "source": [
    "!python ./graphlet_ProtoMAML_label/train.py \\\n",
    "        --data_dir ./data/multiple_graph/cycle/META_LABEL/random_edge100/ \\\n",
    "        --fold_n 1 \\\n",
    "        --k_spt 1 \\\n",
    "        --k_qry 12 \\\n",
    "        --meta_lr 5e-3 \\\n",
    "        --update_lr 0.05 \\\n",
    "        --epoch 10 \\\n",
    "        --update_step_test 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(attention_size=32, data_dir='./data/multiple_graph/cycle/META_LABEL/random_edge100/', epoch=10, fold_n=3, hidden_dim=32, input_dim=1, k_qry=12, k_spt=1, meta_lr=0.01, n_graphlets=5, n_way=2, no_finetune=True, task_num=4, update_lr=0.03, update_step=5, update_step_test=10)\n",
      "100%|████████████████████████████████████████████| 1/1 [00:00<00:00, 596.21it/s]\n",
      "There are 1 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 2/2 [00:00<00:00, 990.62it/s]\n",
      "There are 2 non-isomorphic graphs\n",
      "100%|████████████████████████████████████████████| 4/4 [00:00<00:00, 335.14it/s]\n",
      "There are 6 non-isomorphic graphs\n",
      "100%|█████████████████████████████████████████████| 7/7 [00:00<00:00, 37.09it/s]\n",
      "There are 21 non-isomorphic graphs\n",
      "There are 30 number of graphlets\n",
      "Meta(\n",
      "  (net): Classifier(\n",
      "    (vars): ParameterList(\n",
      "        (0): Parameter containing: [torch.FloatTensor of size 1x32]\n",
      "        (1): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (2): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (3): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (4): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (5): Parameter containing: [torch.FloatTensor of size 32x32]\n",
      "        (6): Parameter containing: [torch.FloatTensor of size 32x30]\n",
      "        (7): Parameter containing: [torch.FloatTensor of size 2x64]\n",
      "        (8): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (9): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (10): Parameter containing: [torch.FloatTensor of size 32]\n",
      "        (11): Parameter containing: [torch.FloatTensor of size 2]\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Total trainable tensors: 4354\n",
      "shuffle DB :train, b:1000, 2-way, 1-shot, 12-query\n",
      "shuffle DB :val, b:100, 2-way, 1-shot, 12-query\n",
      "shuffle DB :test, b:100, 2-way, 1-shot, 12-query\n",
      "epoch: 0 step: 0 \ttraining acc: [0.75000006 0.71875    0.71875    0.71875    0.71875    0.7291667 ]\n",
      "epoch: 0 Val acc: [0.544  0.551  0.5513 0.5527 0.5527 0.5537 0.5527 0.5527 0.5547 0.554\n",
      " 0.555 ]\n",
      "epoch: 0 step: 30 \ttraining acc: [0.8333333 0.8229166 0.8229166 0.8229166 0.8229166 0.8125   ]\n",
      "epoch: 0 step: 60 \ttraining acc: [0.6458334 0.6354167 0.6145834 0.625     0.625     0.625    ]\n",
      "epoch: 0 step: 90 \ttraining acc: [0.75      0.7083333 0.7083333 0.7083333 0.71875   0.71875  ]\n",
      "epoch: 0 step: 120 \ttraining acc: [0.90625006 0.90625    0.90625    0.8854167  0.875      0.8958333 ]\n",
      "epoch: 0 step: 150 \ttraining acc: [0.6875    0.6875    0.6875    0.6979166 0.6979166 0.6979166]\n",
      "epoch: 0 step: 180 \ttraining acc: [0.7083334 0.7083334 0.7083334 0.7083334 0.7083334 0.7083334]\n",
      "epoch: 0 step: 210 \ttraining acc: [0.8229167 0.8333334 0.8333334 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 0 step: 240 \ttraining acc: [0.9375 0.9375 0.9375 0.9375 0.9375 0.9375]\n",
      "epoch: 1 step: 0 \ttraining acc: [0.7708333 0.78125   0.7916666 0.78125   0.78125   0.78125  ]\n",
      "epoch: 1 Val acc: [0.5454 0.545  0.547  0.551  0.551  0.548  0.547  0.547  0.547  0.547\n",
      " 0.546 ]\n",
      "epoch: 1 step: 30 \ttraining acc: [0.7916667 0.8333334 0.8333334 0.8333334 0.8333334 0.8333334]\n",
      "epoch: 1 step: 60 \ttraining acc: [0.90625   0.9166666 0.9166666 0.9166666 0.9166666 0.9166666]\n",
      "epoch: 1 step: 90 \ttraining acc: [0.93749994 0.9479166  0.9479166  0.9479166  0.9479166  0.9479166 ]\n",
      "epoch: 1 step: 120 \ttraining acc: [0.7395833 0.7291666 0.7291666 0.7291666 0.7291666 0.7291666]\n",
      "epoch: 1 step: 150 \ttraining acc: [0.875     0.9166666 0.9166666 0.8958333 0.8854166 0.875    ]\n",
      "epoch: 1 step: 180 \ttraining acc: [0.8958333 0.8541667 0.8541667 0.8541667 0.8541667 0.8645833]\n",
      "epoch: 1 step: 210 \ttraining acc: [0.8229166 0.78125   0.8020833 0.8020833 0.8020833 0.8020833]\n",
      "epoch: 1 step: 240 \ttraining acc: [0.8020834 0.8229167 0.8229167 0.8229167 0.8229167 0.8229167]\n",
      "epoch: 2 step: 0 \ttraining acc: [0.6666667 0.65625   0.65625   0.65625   0.65625   0.65625  ]\n",
      "epoch: 2 Val acc: [0.544  0.5493 0.55   0.5493 0.5464 0.547  0.547  0.5464 0.547  0.547\n",
      " 0.547 ]\n",
      "epoch: 2 step: 30 \ttraining acc: [0.8125    0.8020833 0.8020833 0.8020833 0.8020833 0.8020833]\n",
      "epoch: 2 step: 60 \ttraining acc: [0.93749994 0.96874994 0.96874994 0.96874994 0.96874994 0.96874994]\n",
      "epoch: 2 step: 90 \ttraining acc: [0.8229166 0.8020833 0.8229167 0.8229167 0.8125    0.8125   ]\n",
      "epoch: 2 step: 120 \ttraining acc: [0.9479167 0.9479167 0.9479167 0.9479167 0.9479167 0.9479167]\n",
      "epoch: 2 step: 150 \ttraining acc: [0.875     0.8645833 0.875     0.8854167 0.8854167 0.875    ]\n",
      "epoch: 2 step: 180 \ttraining acc: [0.7604166 0.7395834 0.7395834 0.7395834 0.7395834 0.7395834]\n",
      "epoch: 2 step: 210 \ttraining acc: [0.96875   0.9270834 0.9270834 0.9270834 0.9270834 0.9270834]\n",
      "epoch: 2 step: 240 \ttraining acc: [0.7083334  0.78125    0.78124994 0.7708333  0.7708333  0.7604167 ]\n",
      "epoch: 3 step: 0 \ttraining acc: [0.6979166 0.6979166 0.6979166 0.6979166 0.6979166 0.6979166]\n",
      "epoch: 3 Val acc: [0.5415 0.5474 0.5454 0.547  0.5464 0.5464 0.5464 0.546  0.546  0.5464\n",
      " 0.547 ]\n",
      "epoch: 3 step: 30 \ttraining acc: [0.78125   0.7604167 0.7604167 0.7604167 0.7604167 0.7604167]\n",
      "epoch: 3 step: 60 \ttraining acc: [0.8541666 0.84375   0.84375   0.8541666 0.8541666 0.8541666]\n",
      "epoch: 3 step: 90 \ttraining acc: [0.8645833 0.8645833 0.8645833 0.8645833 0.8645833 0.8645833]\n",
      "epoch: 3 step: 120 \ttraining acc: [0.8333333 0.84375   0.84375   0.84375   0.84375   0.84375  ]\n",
      "epoch: 3 step: 150 \ttraining acc: [0.6770833 0.6979167 0.6979167 0.6979167 0.7083333 0.7083333]\n",
      "epoch: 3 step: 180 \ttraining acc: [0.8333334 0.8229167 0.84375   0.84375   0.8333333 0.8333333]\n",
      "epoch: 3 step: 210 \ttraining acc: [0.84375    0.78124994 0.78124994 0.7916666  0.78125    0.78125   ]\n",
      "epoch: 3 step: 240 \ttraining acc: [0.8854167 0.8854166 0.8958333 0.8958333 0.8958333 0.8958333]\n",
      "epoch: 4 step: 0 \ttraining acc: [0.6770833 0.6979167 0.6979167 0.6979167 0.6979167 0.6979167]\n",
      "epoch: 4 Val acc: [0.5454 0.5493 0.5474 0.547  0.5474 0.5483 0.547  0.547  0.547  0.547\n",
      " 0.547 ]\n",
      "epoch: 4 step: 30 \ttraining acc: [0.87500006 0.8541667  0.8541667  0.8541667  0.8541667  0.8541667 ]\n",
      "epoch: 4 step: 60 \ttraining acc: [0.8229166 0.8125    0.8125    0.8125    0.8125    0.8125   ]\n",
      "epoch: 4 step: 90 \ttraining acc: [0.7916667 0.7916667 0.7916667 0.7916667 0.7916667 0.78125  ]\n",
      "epoch: 4 step: 120 \ttraining acc: [0.9791667 0.96875   0.96875   0.96875   0.96875   0.96875  ]\n",
      "epoch: 4 step: 150 \ttraining acc: [0.7916667 0.7395834 0.7395834 0.7395834 0.7395834 0.7395834]\n",
      "epoch: 4 step: 180 \ttraining acc: [0.87499994 0.8645833  0.875      0.875      0.8541666  0.8541666 ]\n",
      "epoch: 4 step: 210 \ttraining acc: [0.78125   0.7708333 0.7708333 0.7708333 0.7708333 0.7708333]\n",
      "epoch: 4 step: 240 \ttraining acc: [0.8541666  0.84374994 0.84374994 0.84374994 0.84374994 0.84374994]\n",
      "epoch: 5 step: 0 \ttraining acc: [0.74999994 0.7395833  0.7395833  0.7395833  0.7395833  0.7395833 ]\n",
      "epoch: 5 Val acc: [0.543  0.5527 0.547  0.547  0.547  0.547  0.5474 0.548  0.548  0.548\n",
      " 0.548 ]\n",
      "epoch: 5 step: 30 \ttraining acc: [0.78125006 0.7708333  0.7916667  0.7708333  0.7708333  0.7708333 ]\n",
      "epoch: 5 step: 60 \ttraining acc: [0.7291666 0.7395833 0.7395833 0.7395833 0.7395833 0.7395833]\n",
      "epoch: 5 step: 90 \ttraining acc: [0.6875    0.6875    0.6979167 0.6979167 0.6875    0.6875   ]\n",
      "epoch: 5 step: 120 \ttraining acc: [0.9791667 0.9583333 0.9791667 0.9791667 0.9791667 0.9791667]\n",
      "epoch: 5 step: 150 \ttraining acc: [0.8541666 0.8541666 0.8541666 0.8541666 0.8541666 0.8541666]\n",
      "epoch: 5 step: 180 \ttraining acc: [0.78125   0.7916666 0.7916666 0.7916666 0.7916666 0.7916666]\n",
      "epoch: 5 step: 210 \ttraining acc: [0.8541666 0.8645833 0.8645833 0.8645833 0.8645833 0.8645833]\n",
      "epoch: 5 step: 240 \ttraining acc: [0.96875 0.96875 0.96875 0.96875 0.96875 0.96875]\n",
      "epoch: 6 step: 0 \ttraining acc: [0.8645834 0.8645834 0.8645834 0.8645834 0.8645834 0.8645834]\n",
      "epoch: 6 Val acc: [0.542  0.5474 0.5474 0.548  0.547  0.547  0.547  0.547  0.547  0.5464\n",
      " 0.5464]\n",
      "epoch: 6 step: 30 \ttraining acc: [0.8854166 0.8958333 0.8958333 0.8958333 0.8958333 0.8958333]\n",
      "epoch: 6 step: 60 \ttraining acc: [0.7291667 0.7395834 0.7395834 0.7395834 0.7395834 0.7395834]\n",
      "epoch: 6 step: 90 \ttraining acc: [0.78125 0.78125 0.78125 0.78125 0.78125 0.78125]\n",
      "epoch: 6 step: 120 \ttraining acc: [0.8854166 0.8854166 0.8854166 0.8854166 0.8854166 0.8854166]\n",
      "epoch: 6 step: 150 \ttraining acc: [0.65625 0.65625 0.65625 0.65625 0.65625 0.65625]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 6 step: 180 \ttraining acc: [0.9270833 0.9270833 0.9270833 0.9270833 0.9270833 0.9270833]\n",
      "epoch: 6 step: 210 \ttraining acc: [0.65625 0.65625 0.65625 0.65625 0.65625 0.65625]\n",
      "epoch: 6 step: 240 \ttraining acc: [0.7916666 0.7916666 0.7916666 0.7916666 0.7916666 0.7916666]\n",
      "epoch: 7 step: 0 \ttraining acc: [0.8541667 0.8541667 0.8541667 0.8541667 0.8541667 0.8541667]\n",
      "epoch: 7 Val acc: [0.543  0.545  0.543  0.5425 0.5425 0.543  0.5425 0.5425 0.5425 0.5425\n",
      " 0.543 ]\n",
      "epoch: 7 step: 30 \ttraining acc: [0.7604167 0.7604167 0.7604167 0.7604167 0.7604167 0.7604167]\n",
      "epoch: 7 step: 60 \ttraining acc: [0.6875    0.6770833 0.6770833 0.6875    0.6875    0.6875   ]\n",
      "epoch: 7 step: 90 \ttraining acc: [0.84375 0.84375 0.84375 0.84375 0.84375 0.84375]\n",
      "epoch: 7 step: 120 \ttraining acc: [0.75 0.75 0.75 0.75 0.75 0.75]\n",
      "epoch: 7 step: 150 \ttraining acc: [0.7916667 0.8020834 0.8020834 0.8020834 0.7916667 0.7916667]\n",
      "epoch: 7 step: 180 \ttraining acc: [0.9270833 0.8958334 0.9166666 0.9270833 0.9270833 0.9270833]\n",
      "epoch: 7 step: 210 \ttraining acc: [0.78125   0.8020833 0.8020833 0.8020833 0.7395833 0.8020833]\n",
      "epoch: 7 step: 240 \ttraining acc: [0.8125    0.7916667 0.7916667 0.7916667 0.7916667 0.8020834]\n",
      "epoch: 8 step: 0 \ttraining acc: [0.8958333 0.90625   0.90625   0.90625   0.90625   0.9166666]\n",
      "epoch: 8 Val acc: [0.521  0.5156 0.5107 0.519  0.5186 0.5166 0.5156 0.5156 0.5166 0.5156\n",
      " 0.5156]\n",
      "epoch: 8 step: 30 \ttraining acc: [0.8854166 0.8854166 0.8854166 0.8854166 0.8854166 0.8854166]\n",
      "epoch: 8 step: 60 \ttraining acc: [0.9791667 0.96875   0.96875   0.9791667 0.9791667 0.9791667]\n",
      "epoch: 8 step: 90 \ttraining acc: [0.7291666 0.71875   0.71875   0.71875   0.71875   0.71875  ]\n",
      "epoch: 8 step: 120 \ttraining acc: [0.71875   0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 8 step: 150 \ttraining acc: [0.8541667 0.8541667 0.8541667 0.8541667 0.8541667 0.8541667]\n",
      "epoch: 8 step: 180 \ttraining acc: [0.8645833 0.8645833 0.8645833 0.8645833 0.8645833 0.8645833]\n",
      "epoch: 8 step: 210 \ttraining acc: [0.8333333 0.8229167 0.8229167 0.8229167 0.8229167 0.8229167]\n",
      "epoch: 8 step: 240 \ttraining acc: [0.96875   0.9270833 0.9375    0.9479166 0.9479166 0.9583333]\n",
      "epoch: 9 step: 0 \ttraining acc: [0.9479166 0.9583333 0.9583333 0.9583333 0.9583333 0.9583333]\n",
      "epoch: 9 Val acc: [0.544  0.542  0.542  0.542  0.542  0.542  0.5425 0.543  0.543  0.5435\n",
      " 0.5435]\n",
      "epoch: 9 step: 30 \ttraining acc: [0.8541667 0.8645833 0.8541667 0.8541667 0.8541667 0.8541667]\n",
      "epoch: 9 step: 60 \ttraining acc: [0.8020834 0.7916667 0.7916667 0.7916667 0.7916667 0.7916667]\n",
      "epoch: 9 step: 90 \ttraining acc: [0.78125   0.7916667 0.78125   0.78125   0.78125   0.78125  ]\n",
      "epoch: 9 step: 120 \ttraining acc: [0.71875   0.7083333 0.7083333 0.7083333 0.7083333 0.7083333]\n",
      "epoch: 9 step: 150 \ttraining acc: [0.90625 0.90625 0.90625 0.90625 0.90625 0.90625]\n",
      "epoch: 9 step: 180 \ttraining acc: [0.8333333 0.8333333 0.8333333 0.8333333 0.8333333 0.8333333]\n",
      "epoch: 9 step: 210 \ttraining acc: [0.9270833 0.9270833 0.9270833 0.9270833 0.9270833 0.9270833]\n",
      "epoch: 9 step: 240 \ttraining acc: [0.8854167 0.8645833 0.8645833 0.8645833 0.8645833 0.8645833]\n",
      "Test acc: [0.956  0.956  0.9565 0.9565 0.9565 0.9565 0.9565 0.957  0.957  0.957\n",
      " 0.957 ]\n"
     ]
    }
   ],
   "source": [
    "!python ./graphlet_ProtoMAML_label/train.py \\\n",
    "        --data_dir ./data/multiple_graph/cycle/META_LABEL/random_edge100/ \\\n",
    "        --fold_n 3 \\\n",
    "        --k_spt 1 \\\n",
    "        --k_qry 12 \\\n",
    "        --meta_lr 1e-2 \\\n",
    "        --update_lr 0.03 \\\n",
    "        --epoch 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
